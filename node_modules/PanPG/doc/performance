The goal currently is to parse jQuery in reasonable time and memory.

jQuery 1.3.2 is 120762 characters.

So we will plan in 124000 characters in our estimates.

Currently about 1GiB of memory is used when attempting to parse this, and the parse does not succeed (or I have never waited long enough).

Allocating 124k empty objects in an array (as we do for the state table) allocates about 4MiB in spidermonkey, so that's not the problem.

js> gc()
before 184320, after 61440, break 080f1000
js> for(var i=0;i<124000;i++)a.push({})
124000
js> gc()
before 4177920, after 4177920, break 08899000

We next create a 124000 character string, and then attempt to populate the table with tails of the string:

js> s=Array(124001).join('.')
............................... [...]
js> for(var i=0;i<124000;i++)a[i].str=s.slice(i)

Soon after we kill the process...

I had hoped that slice() would create a reference to an offset into an existing string but apparently this is not how it is implemented, and an actual copy is made.
Well, perhaps that actually is how it's done:

js> a=[]

js> for(i=0;i<4096;i++)a[i]={}
[object Object]
js> a.length
4096
js> s=Array(4097).join('.');s.length
4096
js> gc()
before 155648, after 155648, break 080d8000
js> for(i=0;i<4096;i++)a[i].str=s.slice(i)
.
js> gc()
before 196608, after 196608, break 080f9000
js> delete a; delete s
true
js> gc()
before 196608, after 28672, break 080f9000
js> a=[];for(i=0;i<16384;i++)a.push({})
16384
js> s=Array(16385).join('.');s.length
16384
js> gc()
before 569344, after 569344, break 081da000
js> for(i=0;i<16384;i++)a[i].str=s.slice(i)
.
js> gc()
before 716800, after 716800, break 0825e000
js> 

This is bizarre and seems incredibly non-linear

js> gc()
before 20480, after 20480, break 08094000
js> s=Array(32769).join('.');s.length
32768
js> gc()
before 20480, after 20480, break 08094000
js> a=[];for(i=0;i<32768;i++)a[i]={str:s.slice(i)}
[object Object]
js> gc()
before 1409024, after 1409024, break 39b97000
js> countHeap()
65902
js> // the OS reports that the process has ballooned to 801MB
js> delete s
true
js> gc()
before 1409024, after 1409024, break 39bb7000
js> delete a
true
js> gc()
before 1409024, after 24576, break 32ab0000
js> countHeap()
362
js> 

Fixing this so that the slices are not saved reduced the memory usage even on moderate (16k) inputs by a huge amount, from a Firefox process size of around 600MiB to staying around 200MiB (which is where it started).
We can now actually parse jQuery.
Memory usage is reasonable.
jQuery v1.3.2, minified (57254 characters) parses in 192.798s.
The un-minified version of the same file (120762 characters) parses in 321.656s.

This is unacceptably slow, but it's also much better than it was.
Next I will add some profiling information to the generated code.

20090906

jQuery v1.3.2 does not parse at all using the ES5 parser, as it contains a function getWH() which is inside a block in an if statement, which is not permitted by the grammar.

If this function is moved outside of the if statement, the entire file successfully parses in 29.6 seconds.
This is on the same hardware as the earlier 321.7s result, so we have a speedup of about 10 times, consistent with the benchmarking done on much smaller inputs and simpler grammars.

Some further optimization of what gets added to the result table brings our time to parse jQuery down to 17.1s.
However it should be noted that the current parser is no longer constructing a parse tree, this is a separate step which needs to be added in again.
Parsing jQuery and building the tree now takes about 20s.

Using integer indices (of the rule's position in the given PEG) rather than rule names, and arrays rather than objects, in the result table, reduces the runtime to about 17s for the same task of parsing and building a tree for jQuery (but not returning the result from the worker).
Of this, building the result tree, which is a separate step at the end, takes only an additional second or so.


20100219
--------

With debugging output and assertions turned on:

v6 default:
 rate: 0.0003129 ms: 3196 max: 3196 min: 3196 n: 1

rate is in KiB/ms, so a rate of 1.0 is 1MiB/sec

With debugging output and assertions turned off:

v6 default:
 rate: 0.01575 ms: 127 max: 65 min: 62 n: 2 

So the debugging output imposes a performance penalty of approximately 50x

2KiB:

test chars (charCodeAt):
rate: 0.6863 ms: 2002 max: 9 min: 0 n: 1374
streaming fast:
rate: 0.0001790 ms: 5587 max: 5587 min: 5587 n: 1
streaming v6:
rate: 0.001115 ms: 2691 max: 920 min: 865 n: 3
streaming v6 hacked:
rate: 0.001142 ms: 2627 max: 912 min: 850 n: 3
v6 default:
rate: 0.0008760 ms: 2283 max: 1158 min: 1125 n: 2
state pushes and pops:
rate: 0.05868 ms: 2011 max: 31 min: 12 n: 118

4KiB:

test chars (charCodeAt):
rate: 0.4765 ms: 4000 max: 10 min: 1 n: 1906
streaming fast:
rate: 0.0004070 ms: 7371 max: 4499 min: 1429 n: 3
streaming v6:
rate: 0.0007854 ms: 5093 max: 1302 min: 1240 n: 4
streaming v6 hacked:
rate: 0.0007962 ms: 5024 max: 1280 min: 1247 n: 4
v6 default:
rate: 0.0003496 ms: 5721 max: 3946 min: 1775 n: 2
state pushes and pops:
rate: 0.04002 ms: 4023 max: 42 min: 23 n: 161

8KiB:

test chars (charCodeAt):
rate: 0.1524 ms: 8001 max: 3051 min: 3 n: 1219
streaming fast:
rate: 0.0002063 ms: 9696 max: 6856 min: 2840 n: 2
streaming v6:
rate: 0.0003940 ms: 10152 max: 2583 min: 2503 n: 4
streaming v6 hacked:
rate: 0.0004075 ms: 9817 max: 2466 min: 2441 n: 4
v6 default:
rate: 0.0001000 ms: 9999 max: 9999 min: 9999 n: 1
state pushes and pops:
rate: 0.02041 ms: 8034 max: 72 min: 46 n: 164

test chars (charCodeAt):
rate: 0.2475 ms: 8003 max: 18 min: 3 n: 1981
streaming fast:
rate: 0.0002258 ms: 8857 max: 5983 min: 2874 n: 2
streaming v6:
rate: 0.0003903 ms: 10248 max: 2607 min: 2536 n: 4
streaming v6 hacked:
rate: 0.0004008 ms: 9981 max: 2530 min: 2469 n: 4
v6 default:
rate: 0.0001008 ms: 9918 max: 9918 min: 9918 n: 1
state pushes and pops:
rate: 0.02043 ms: 8028 max: 69 min: 46 n: 164

Conclusion: Performance goes down as total document size goes up, because in v6 with default flags, we're not actually doing any streaming output.
We're not yet doing the analysis that would allow it, so unless the flags are passed in by the user, the performance will be bad and memory usage high because of the caching of results (even compared to the streaming parser which is already slow).

Generating the ES5 parser in the v6 streaming codegen (with default flags) took 85.402s.
This can also be improved, but isn't as much of a priority.
We currently make a lot of passes over the ruleset to generate the parser.
Looking into this showed that almost all the time is spent in the TAL program which attaches re objects to the PEG AST.
This is probably almost all due to the inefficiency of the current TAL implementation, which is very much unoptimized and optimized for correctness and development time over run-time performance.
Probably TAL will change a lot before settling on a more practical semantics, and then an efficient implementation may be worth writing.