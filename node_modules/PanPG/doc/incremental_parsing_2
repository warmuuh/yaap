Date: 2009-12-16

Before tackling full incremental parsing in the new codegen we need to support simple streaming parsers.

Here is a parser for arithmetic expressions from the current (v5) codegen:

function p_arith_Expr(str){
 var tbl=[],pos=0,l=str.length+1;while(l--)tbl.push([]);l=str.length;
 function Expr(a){var x,p=pos,c;if(x=tbl[p][0]){pos=x[1];a.push([p,0]);return 1}if(x==false){return 0}c=[];return fin(c,p,0,_Expr(c),a)}
 function Add(a){var x,p=pos,c;if(x=tbl[p][1]){pos=x[1];a.push([p,1]);return 1}if(x==false){return 0}c=[];return fin(c,p,1,_Add(c),a)}
 function Mult(a){var x,p=pos,c;if(x=tbl[p][2]){pos=x[1];a.push([p,2]);return 1}if(x==false){return 0}c=[];return fin(c,p,2,_Mult(c),a)}
 function Num(a){var x,p=pos,c;if(x=tbl[p][3]){pos=x[1];a.push([p,3]);return 1}if(x==false){return 0}c=[];return fin(c,p,3,_Num(c),a)}
 function S(a){var x,p=pos,c;if(x=tbl[p][4]){pos=x[1];a.push([p,4]);return 1}if(x==false){return 0}c=[];return fin(c,p,4,_S(c),a)}
 var _Expr=Add
 var _Add=q(Mult,r(0,0,q(r(0,1,S),sl_0,r(0,1,S),Mult)))
 var _Mult=q(Num,r(0,0,q(r(0,1,S),sl_1,r(0,1,S),Num)))
 var _Num=o(sl_2,q(cs_0,r(0,0,cs_1)))
 var _S=r(1,0,cs_2)
 function cs_0(){var c,x;if(pos==l)return false;c=g(pos);x=c<49?0:c<58?1:0;if(x){pos++;return true}return false}
 function cs_1(){var c,x;if(pos==l)return false;c=g(pos);x=c<48?0:c<58?1:0;if(x){pos++;return true}return false}
 function cs_2(){var c,x;if(pos==l)return false;c=g(pos);x=c<32?0:c<33?1:0;if(x){pos++;return true}return false}
 function sl_0(){var p=pos;if(str.charCodeAt(p)==43){pos+=1;return true}return false}
 function sl_1(){var p=pos;if(str.charCodeAt(p)==42){pos+=1;return true}return false}
 function sl_2(){var p=pos;if(str.charCodeAt(p)==48){pos+=1;return true}return false}
 function fin(c,p,x,r,a){if(r)a.push([p,x]);tbl[p][x]=r?[true,pos,c]:false;return r}
 function e(){return true}
 function o(){var args=arguments;return function(c){var i,l;for(i=0,l=args.length;i<l;i++)if(args[i](c))return true;return false}}
 function q(){var args=arguments;return function(c){var i,l,cp=pos,cl=c.length;for(i=0,l=args.length;i<l;i++)if(!args[i](c)){pos=cp;t(c,cl);return false}return true}}
 function r(m,n,f){return function(c){var i=0,cp=pos,cl=c.length;while(i<m){i++;if(!f(c)){pos=cp;t(c,cl);return false}}cl=c.length;while(i++<n||n==0)if(!f(c))return true;return true}}
 function n(f){return function(){var p=pos,x=f([]);pos=p;return !x}}
 function p(f){return function(){var p=pos,x=f([]);pos=p;return x}}
 function t(a,n){if(a.length&gt;n)a.splice(n)}
 function g(p){return str.charCodeAt(p)}
 function b(p,n){var x=tbl[p][n],c=[],a=[n,x[1]-p,c],y=x[2],i=0,l=y.length,z;for(;i<l;i++){z=y[i];if(z[0]&gt;p)c.push([-1,z[0]-p]);c.push(b(z[0],z[1]));p=tbl[z[0]][z[1]][1]}if(p<x[1]&amp;&amp;c.count)c.push([-1,x[1]-p]);return a}
 return Expr([])&amp;&amp;pos==l?[true,b(0,0)]:[false,pos,tbl]}
p_arith_streaming_Expr.names=['Expr','Add','Mult','Num','S'];

The grammar:

Expr ← Add
Add ← Mult ( S? "+" S? Mult )*
Mult ← Num ( S? "*" S? Num )*
Num ← "0" / [1-9] [0-9]*
S ← [ U+0020 ]+

// note that we mutate tbl rather than create a new array

since we need to be resumable we can no longer rely on recursive functions to maintain the stack
instead of calling Expr we push Expr's index (0) onto the stack
when we re-enter on each chunk we look at the stack to see what we need to try to parse
our saved state consists of the result table and the call stack which we manually maintain

Transforming the recursive function version into a version with an explicit call stack requires analyzing all the shared state in the call stack on which we rely and would need to reconstruct to resume.

Assume the input "4*3+" has been seen in the first chunk.
Considering only the parse rules on the stack, we have:
 Expr
  Add
   Mult
    Num    [E A M N]
     "4"
    .      [E A M]
    S?     (S fails, S? succeeds)
    "*"
    S?
    Num
     "3"
    .
    (S? "*" S? Num) is tried again, but fails since there is a "+" not a "*"
    The *-expr, and Mult, succeed, without seeing the end of the input chunk [1]
   .       [E A]
   We now push Mult onto the stack of nodes we can safely return in this chunk [2]
   S?
   "+"     succeeds!
   S?      S fails, but touching EOF, this is the first time EOF is touched.

We don't know whether the next input char will be a space or not, so we cannot go any further.[3]

[1]
We have no way of knowing a priori when (S? "*" S? Num) fails the second time, that it will never succeed because of the "+" already seen.
Instead we must rely on the chunk annotations given by the user to make these determinations.

When the next chunk is seen, do we need to test the (S? "*" S? Num) again?
No, because it didn't read forward to the end of the input before it failed.
Since it failed when the "+" was seen, it would be fine to resume from this point.
However, that is just a *-expr, not a named rule, so in this case there won't be any cached result on the result table.

[2]
This is because: (1) Mult succeeded without seeing EOF, and (2) Mult is in the "chunkable" set given by the caller.

[3]
In particular, we don't even know if the Add or Expr nodes will succeed.
The price of streaming output is that errors can happen at any time.

At the moment we first touch EOF, there are two rules, Expr and Add on the stack.
Looking at the JavaScript stack instead of only the parse rule stack, we have the following functions:

 function Expr(a){var x,p=pos,c;if(x=tbl[p][0]){pos=x[1];a.push([p,0]);return 1}if(x==false){return 0}c=[];return fin(c,p,0,_Expr(c)
                                                                                                                                   ^
 waiting for _Expr(c) to return, then fin() will be called.
 In scope are the lookup result x (useless),
 the child nodes so far `c`, which has been passed down to _Expr(),
 the p=pos which was the start position and will be passed to fin()

 var _Expr=Add
              ^
 var _Add=q(Mult,r(0,0,q(r(0,1,S),sl_0,r(0,1,S),Mult)))
                                                       ^ (waiting for 'q' to return)

 Here is the interesting stuff.
 The outer q() has been called and all the combinator functions inside it have been called and have returned new parser functions which have been passed to q().
 The call structure here just reflects the grammar and holds no interesting state, for that we must look at q() or rather the anonymous function which q() has returned

 function q(){var args=arguments;return function(c){var i,l,cp=pos,cl=c.length;for(i=0,l=args.length;i<l;i++)if(!args[i](c)){pos=cp;t(c,cl);return false}return true}}
                                                                                                                              ^ waiting for args[i](c)

 In this case args[i] happens to be the function corresponding to ( S? "+" S? Mult )*
 It was returned from the call to r() above, r(0,0,q(r(0,1,S),sl_0,r(0,1,S),Mult))
 Looking at the call to q() we note the following state:
 i represents the position in the sequence, in this case Mult · ( S? "+" S? Mult )*
 cp is the original character position which we need to backtrack to in case the entire thing fails
 cl is the original length of the child nodes, which we need to truncate it to if the entire thing fails
 It is also worth noting that the arguments to q, i.e. the args variable, is closed over and forms part of the unique identity of what is currently on the stack.

 function r(m,n,f){return function(c){var i=0,cp=pos,cl=c.length;while(i<m){i++;if(!f(c)){pos=cp;t(c,cl);return false}}cl=c.length;while(i++<n||n==0)if(!f(c))return true;return true}}
                                                                                        ^ waiting for f(c) to return

 In the anonymous function returned from this call to r() we are waiting for f(c), where f is the function returned by r(0,1,S)
 This corresponds to the position ( S? "+" · S? Mult )* inside the first repetition of this *-expr
 i represents the number of matches and would be needed to know how many times to keep going in case this was an upper-bounded rep
 We could do away with this by unrolling all (m,n) rep into sequences of length m followed by either *-expr or nothing.
 cp is the original char pos and is needed to reset in case the subexpr fails before m is reached.
 Otherwise, or with the transformation above, it would not be needed since *-expr never rolls back the pos (though its sub-expr may)
 cl is used to truncate the child list in case this fails before m is reached
 It is also assigned to near the end of the function for a reason I do not understand.
 It cannot possibly be used after it is assigned at that point so that assignment should be removed.

 function r(m,n,f){return function(c){var i=0,cp=pos,cl=c.length;while(i<m){i++;if(!f(c)){pos=cp;t(c,cl);return false}}cl=c.length;while(i++<n||n==0)if(!f(c))return true;return true}}
                                                                                        ^ waiting for f(c)

 The next call is also an anonymous function returned from r, in this case r(0,1,S) as mentioned above.
 This corresponds to the S? expression.
 It is also waiting for f(c) which in this case is the S expression and function
 i = 0, since the S has not yet succeeded.
 cp and cl are at their initial values.

 function S(a){var x,p=pos,c;if(x=tbl[p][4]){pos=x[1];a.push([p,4]);return 1}if(x==false){return 0}c=[];return fin(c,p,4,_S(c),a)}
                                                                                                                              ^
 waiting on its actual test function

 var _S=r(1,0,cs_2)
                   ^ waiting on r()

 This corresponds to the S rule's RHS, which is " "+ i.e. 1 or more spaces.

 function cs_2(){var c,x;if(pos==l)return false;c=g(pos);x=c<32?0:c<33?1:0;if(x){pos++;return true}return false}
                                               ^

 Here pos==l because we are at the end of the chunk, and so we return false.
 This is where some magic needs to happen.

At any given time, conceptually, there is a stack of some number of the following calls with their corresponding operators and local state:

string   sl_n    "..."    start_pos
cset     cs_n    [...]    -

sequence   q      A B     position within the sequence, start_pos, length of the current child array at the start
rep        r      A*      number of subexpr matches so far
ordC       o     A / B    position within the sequence of choices
posL       p      &A      original child array, original position
negL       n      !A      original child array, original position

When an expression is entered, we create a 'stack frame' for it by pushing onto the array that represents the stack.

Global state includes the current position, the reference to the current child array, and the result table.

We define a core PEG language which translates string literals into sequences of individual characters and all repetitions other than *-exprs into combinations of sequences, ϵ, and *-exprs.
The only elements of the core PEG language are sequence, greedy 0-or-more repetition, ordered choice, negative and positive lookaheads, and primitive tests on single characters.

For clarity we use S-expressions to write the core language, and use the abbreviations above for sequences (q), repetitions (r), ordered choice (o), and positive and negative lookahead (p and n).
Tests on characters are written as string literals or ranges.

After translation to the core language, the arithmetic grammar becomes:

Expr ← Add
Add ← (q Mult (r (q (o S ϵ) "+" (o S ϵ) Mult)))
Mult ← (q Num (r (q (o S ϵ) "*" (o S ϵ) Num)))
Num ← (q [0-9] (r [0-9]))
S ← (q " " (r " "))

random thought: there's an interesting duality between tail-call elimination and eliminating co-extensive parent nodes

We associate each expression with a distinct integer which we call a state.
When parsing, the parser moves through these states, pushing them onto the stack and popping them as expressions succeed or fail.
Each expression has an expression type, which is either q, r, o, p, or n, that is, the composite expressions of the PEG language, or a primitive expression, or a rule.
Here for clarity we number these starting at 0x00 for the first rule, 0x10 for the expressions within the second rule, etc.
In some places we need to get the rule index from the state integer.
Currently we do this by dividing, but in a real generated parser we can save having to do this (and having to distribute states evenly) by using the rule index as the start state for that rule, and then using states higher than the highest rule index for all other states.

Expr ← Add
      0                                          + 0x00

Add ← (q Mult (r (q (o S ϵ) "+" (o S ϵ) Mult)))
     0  1    2  3  4  5 6  7   8  9 A  B         + 0x10

Mult ← (q Num (r (q (o S ϵ) "*" (o S ϵ) Num)))
      0  1   2  3  4  5 6  7   8  9 A  B         + 0x20

Num ← (q [0-9] (r [0-9]))
     0  1     2  3                               + 0x30

S ← (q " " (r " "))
   0  1   2  3                                   + 0x40

In addition to these states there are test states, which represent that the parser has walked through the grammar to the point that it now knows which test to perform on the character at the current position.
Here we number the test states starting from 0x80.

0x80  ϵ  // aka 'true'
0x81  "+"
0x82  "*"
0x83  [0-9]
0x84  " "

All of these tests consume one input character when they succeed, except for ϵ which always succeeds and consumes no input.

Based on the key above, we construct the following three tables which determine the next state when an expression is entered (the T table), when it matches (M), and when it fails (F).

 //     0    1    2    3    4    5    6    7    8    9    A    B    C    D    E    F
 ,T=[0x10,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x0_  (Expr)
    ,0x11,0x20,0x13,0x14,0x15,0x40,0x80,0x81,0x19,0x40,0x80,0x20,  -1,  -1,  -1,  -1 // 0x1_  (Add)
    ,0x21,0x30,0x23,0x24,0x25,0x40,0x80,0x82,0x29,0x40,0x80,0x30,  -1,  -1,  -1,  -1 // 0x2_  (Mult)
    ,0x31,0x83,0x33,0x83,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x3_  (Num)
    ,0x41,0x84,0x43,0x84,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1]// 0x4_  (S)
 ,M=[ 257,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x0_  (Expr)
    , 257,0x12, 257,0x13,0x17, 257, 257,0x18,0x1B, 257, 257, 257,  -1,  -1,  -1,  -1 // 0x1_  (Add)
    , 257,0x22, 257,0x23,0x27, 257, 257,0x28,0x2B, 257, 257, 257,  -1,  -1,  -1,  -1 // 0x2_  (Mult)
    , 257,0x32, 257,0x33,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x3_  (Num)
    , 257,0x42, 257,0x43,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1]// 0x4_  (S)
 ,F=[ 256,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x0_  (Expr)
    , 256, 256,  -1, 257, 256,0x16,  -1, 256, 256,0x1A,  -1, 256,  -1,  -1,  -1,  -1 // 0x1_  (Add)
    , 256, 256,  -1, 257, 256,0x26,  -1, 256, 256,0x2A,  -1, 256,  -1,  -1,  -1,  -1 // 0x2_  (Mult)
    , 256, 256,  -1, 257,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1 // 0x3_  (Num)
    , 256, 256,  -1, 257,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1]// 0x4_  (S)

We also have a table of expression types, which are either q, r, o, n, p, for the composite PEG expressions, R for nonterminals (rules), or L for literals, i.e. primitive expressions which actually test the input.

 //     0123456789ABCDEF
 ,type="R               " // 0x0_ (Expr)
      +"qRrqoRLLoRLR    " // 0x1_ (Add)
      +"qRrqoRLLoRLR    " // 0x2_ (Mult)
      +"qLrL            " // 0x3_ (Num)
      +"qLrL            " // 0x4_ (S)

We have a table of anonymous expressions which create an anonymous node in the resulting parse tree.
These are determined partly from the original grammar and extended by annotations the user provides which can cause even named rules to generate anonymous nodes.
This is just a bitfield, though we use a string here for clarity.

 //     0123456789ABCDEF
 ,anon="0               " // Expr
      +"000000010000    " // Add
      +"000000010000    " // Mult
      +"0000            " // Num
      +"0000            " // S

The only anonymous nodes generated here are the operators "*" and "+".
The rule for generating these automatically is approximately the same as the rule for generating anonymous nodes in the v5 parsers.
One difference is that, in some cases, more expressions may need to be marked as anonymous, and then trimmed or combined based on the matches that actually occur (or the combining might not even need to happen).
Anonymous nodes are somewhat comparable to text nodes in DOM, with the combination being akin to normalization.
The rule for using these in the parser is that expressions appearing in anon always create an anonymous node and has no children.
For all other expressions, only named rules generate nodes.

There is also a table of expressions which preclude streaming output.
Expressions for which this bit is set will prevent the parser from emitting any nodes until the entire expression succeeds or fails.
Here the expressions for which partial emitting is unsafe are determined by looking at the grammar.
In the actual code generator, this set of expressions will be provided as an argument.
We only need 0 and 1, but we also use " " to mean "this will never be accessed" and "-" to mean "this will never have any effect on output" and we use assertions to verify these.

 //         0123456789ABCDEF
 ,nostream="0               " // Expr 0x00
          +"00-01--00000    " // Add  0x10
          +"00-01--00000    " // Mult 0x20
          +"0000            " // Num  0x30
          +"0000            " // S    0x40

Can these expressions be streamed, and why?
Note that we assume the expression that called the current rule can be streamed, otherwise the parser is already in "no-streaming mode" and the bit has no significance.

Add ← (q Mult (r (q (o S ϵ) "+" (o S ϵ) Mult)))
     0  1    2  3  4  5 6  7   8  9 A  B         + 0x10

0: yes (if some parts of a q-expr can't be streamed we can mark them separately)
1: yes - in this case if a Mult succeeds there is no way for the entire expression to fail (unless the entire parse fails)
2: yes, r cannot fail and can always be streamed
3: yes, by q-rule given above
4: no; if the (o S ϵ) succeeds and matches an S, we will eventually emit an S (or fail the entire parse) but not necessarily as part of this expression.
5: yes, if the S succeeds the o will succeed.
6: yes, but irrelevant as ϵ never fails and never emits anything.
but note that both 5 and 6 are irrelevant as we are already in non-streaming mode because of 4.
7: yes, if the "+" succeeds we are committed to either matching the rest of the Add rule, or failing the entire parse, so at this point the previous children of the rule and this anonymous node can all be emitted.
8: yes, since we are in a "committed sequence" there is no way for this to fail without the entire rule (and hence the entire parse) failing.
9: yes
A: yes
B: yes, since it is the last expression (of the last expression...) of the rule.

Num and S are interesting cases:
Num ← (q [0-9] (r [0-9]))
     0  1     2  3                               + 0x30
S ← (q " " (r " "))
   0  1   2  3                                   + 0x40
Because the question of whether they can be streamed or not does not matter, since they have no child nodes.
If we were to parse an entire large file containing only whitespace, the difference would be in whether the start of the S node would be emitted at the beginning, followed be the close and length at the end, or whether nothing would be emitted until the end.

A note about composability: here we are assuming that 'failing to parse' includes a successful parse which does not consume the entire input.
If two complete grammars are composed as a sequence (e.g. "parse an integer followed by a string" or "parse a CSS selector followed by a JavaScript program") without consideration for these effects, then any commit points like these in the first grammar would be invalidated, since the grammar could run ahead into the second part of the input and then fail, without the entire parse failing.
A more direct way to say this is simply that the considerations of what emit points are safe depend on global knowledge about the grammar.
...as far as I can tell.

When the parser enters an expression it pushes a frame onto the stack.
The contents of that frame depend on the type of the expression.
The frame will be used when the result of the expression is known.
The result can be either success or failure.
(There is also the case of reaching the end of the current chunk, but this does not affect the stack.)
The frame that is pushed onto the stack for an expression then needs to include enough information to continue apppropriately after either result.

Currently we push S onto the stack when a new state is entered, so that there is always the current state in S, and the previous state in the frame on the top of the stack.
Instead we could push the current state onto the top of the stack, which would probably be more intuitive and might simplify the code.
I'm not really sure what this would entail.
For one thing, there would need to be a frame for the start rule already on the stack when the parser is started, since we never actually enter this rule, but then is probably necessary anyway unless there is going to be some special-case machinery to add the start rule to the returned tree (in which it currently does not appear).
When the result of an expression is known, currently the expression that succeeded is already in S, either because the primitive testing code put it there (by popping the stack, but this could just as well be saved an a variable for that purpose), or because the previously unwound frame in the previous iteration of the has_result loop put it there.
Instead of that we would need to get it from the stack frame, but the same number of stack pushes and pops would happen, so this seems a largely irrelevant difference except in terms of code readability and complexity.
On the other hand it is quite natural to see the stack as a stack of states that we are in, in addition to the current state in S.

For each expression type, here are the things that we push onto the stack, and what happens to them on success or failure:
q       previous S, previous child array, start pos
Success: child array is appended to parent child array, start pos is discarded
Fail: current child array is discarded, position is reset to start pos
Success happens when the last subexpr succeeds, failure when any fails.
In either case the parent child array becomes current.

r       S, child array
Success: child array is appended to parent child array
Fail: r cannot fail
Success happens when the subexpr fails.  If it succeeds we loop.
Since this cannot backtrack we do not save the start pos; note that the subexpression may of course backtrack.

o       S
Success: append the sub-expression's child array to the parent child array.
Fail: do nothing
Success happens when any subexpression succeeds, failure when the last fails.
There is no need to create a separate child array because each subexpr will create its own, and as soon as any subexpr succeeds we are done.
In other words, unlike with q, the child array is not built up by parts, but is received atomically from the subexpr which succeeds.
For the same reason there is no need to store the start pos because this will never need to be rolled back.
In the success case, appending the child array actually doesn't need to be done explicitly, since no new child array is created when the o is entered.

n       S, child array, pos
p       S, child array, pos
On either success or failure, the child array of a lookahead is thrown away and the pos is rolled back.
Then the result is inverted in the case of n, and the stack popped as usual in either case.
The result inversion actually happens in the M and F tables, the code for handling these expressions is the same.

R       S, child array, pos
On entry, we also check the result table, and if a result exists, use it instead.
Success: push the child array onto a new rule match node and push this node onto the parent
Fail: drop the child array
Either success or failure is recorded in the result table.
"Pushing onto the parent" means, in an event stream model, emitting a rule start event, all child events, and a rule end event.
If the rule was entered in streaming mode however, some of these events may have already been sent.
If events have already been sent and the rule fails then this is a hard failure.
If there is a parent position in the stack which has a failure continuation, some branch of which could have succeeded, then this is an error.
This can be tested by reparsing in non-streaming mode and comparing the output, it is acceptable if a different error is generated but if the non-streaming parser succeeds then it is a bug (possibly in the grammar annotations provided by the caller).
In a Rule expression it is never necessary to roll back to the start position, since any subexpression which failed will have done so; note that a rule only has one direct subexpression (which may be a sequence).
However, we do store the position, so that once the result is known we can add it to the result table, which is indexed by position then by rule index, and so that we can report the length of the match in the output stream.
If the rule was entered in streaming mode, this represents an assertion by the provider of the streaming annotations that the rule, if successful, will never be backtracked over by a parse that will succeed, though it may be retested by a parse that ultimately will fail.
This implies that the rule need not be recorded in the result table, even if the result is not being streamed immediately.
Instead we should record a distinguished value indicating that the result has been 'burned' by a commit point annotation.
If such a result is accessed in the table, the accessor can then either throw an exception, or return a parse failure, possibly with a warning.
In 'grammar annotation debugging mode' the warning would be useful.
There might be stricter annotations that would also make the exception correct, e.g. if there is a 'backtracking will not occur' annotation on an expression rather than the weaker 'commit point' annotation.

Primitive tests (i.e. in this version, only cset membership tests) do not push data onto to the stack, though they can be conceptualized as a stack frame which is simply optimized down to nothing but a state change.
They are the only thing which consumes input, and thus changes the parser from the "control" or "stack setup" mode to the "has result" mode in which stack frames are popped and parse results are determined.
Once in 'has result' mode, multiple state changes may occur, shrinking the stack and remaining in 'has result' mode, until a state is reached which again enters "control" mode (i.e. an expression state) or primitive testing mode.
There are three basic 'modes' in the code, the first two controlled by a test on the state value and the last one controlled by whether there is a result.
The first mode is 'expression entry' mode, in which stack frames are set up and the state is advanced.
In this mode, an expression is being entered and the result of the expression is not yet known.
If there is an entry for this expression at this position in the result table, the parser will proceed directly to the has result state, but otherwise, after one or more expressions have been entered, a primitive expression will be entered, and a primitive test state will be the current state.

A primitive test either succeeds or fails, and may increment the position.
In the case of ϵ, the position is not incremented, otherwise the position is incremented for every test which succeeds.
Once a test succeeds or fails we have a result value, and are in 'has result' mode.
The primitive test code also takes care of popping one frame, getting back to the primitive expression state, and then getting the next state or result from the M or F table as appropriate.
After the primitive test, either another expression state will be entered, or one or more expressions will be exited by the has_result loop.

In has_result mode, we deal with the teardown of expressions that have been entered and for which the result is now known.
The result applies to the expression currently at the top of the stack, which is first popped.
This will not be a primitive expression, but always a q, r, o, p, n, or R expression.

It is possible to streamline lexing by having primitive test states that have their own follow states in the M or F tables.
Then in the primitive test code, we can look up the result and state in the M or F table, and if the new state is a primitive state, then it is immediately tested and the stack is unchanged.
This isn't implemented currently.

Generating anonymous nodes
--------------------------

Our parse tree result format is very minimal, representing each node as only a rule name and a length.
This means that in order to determine where in the input a node corresponds to, it is necessary to keep track of the lengths as a running sum while walking the tree.
This in turn means that there cannot be any segment of the input which is unaccounted for by some leaf node; the leaf nodes must tile the input.
This is described in further detail in parse_tree_representation.
If the input "2+2" is matched by the grammar above, then, we have three leaf nodes in the result, all of length 1.
They are the Num nodes matching each of the "2"s, and an anonymous node between them.
If this anonymous node were dropped it would be impossible to determine the correct location of the second Num match.
Currently, primitive expressions such as "2" do not store any data of interest on the stack, this means that determining the location of the match from the stack alone could be difficult.
The result table contains enough data to reconstruct it, since each anonymous node has at least one named-rule sibling node.
However we probably prefer to get this data from the stack for simplicity.
We can easily determine from the grammar alone where anonymous nodes will be generated.
When entering an expression that can generate an anonymous node, then, we can simply add the start position to the corresponding stack frame, and when exiting the expression, emit the anonymous node with the length given by subtracting from the current position.
If the anonymous node is empty, however, i.e. if the start and end positions are the same, we omit it entirely from the output.

It should be noted that, based on annotations provided to the code generator, not only primitive expressions but also named rules that are deemed uninteresting by the user can be supressed, generating only anonymous nodes in the output.


streaming output
----------------

The output of a streaming parser must be sent in chunks even while inside an outer rule (most obviously, the start rule itself) the final result of which is not yet known.
How does the parser know when a node can be safely emitted?
For this we can rely on annotations provided by the caller when the parser is generated, on runtime data gathered during the parse, on rules based on expression types, and even potentially on heuristics which can be used to support or disprove the annotations the caller provides.

Nodes are generated only by rule matches and by anonymous expression matches.
When a node is generated, whether the node can be emitted depends on the parent expressions on the call stack, and of course on the rest of the input, but we can make no assumptions about that.

An expression can make it unsafe for a rule to be emitted, but cannot make it safe if a parent expression has already made it unsafe.
The interesting question then is whether an expression preserves this safety, or streamability.

Assuming a rule R has just been entered,

R ← e

And that R is safe to stream, meaning that when R succeeds the corresponding node can be emitted immediately.

Obviously, when e itself is successful, R's success is known.
So if e is a named reference to another rule or a primitive expression, it succeeds or fails atomically and there is no interesting question for streaming.
In the case of n or p, nothing is emitted in any case, so these are also uninteresting.
The interesting cases are when e is a q, r, or o expression.

In the case of (q a b), generally, it is not safe to emit a until b also succeeds.
In the case of (r a), it is safe to emit each a as it succeeds, since r never backtracks.
In the case of (o a b), it is safe to emit the result of the first branch which succeeds, since no other will be tried.

So, actually the only interesting case is q expressions, in which subexpressions may succeed but later be retracted due to the entire sequence failing.
The interesting subexpressions of the q are all but the last.
So for each sequence in a grammar, we can say whether each subexpression is safe or unsafe, possibly at some point within that subexpression.
Essentially we are saying that we are committed, at some point, to either succeeding on that branch or else failing the entire parse.
This has a close connection to LR(n) languages.
If the language is LR(1), for example, then after the next character plus one is consumed, the branch is committed and will succeed or fail with the entire parse.
Or rather, we can say that if the language is LR(1), a PEG could be constructed for it such that the above holds; it does not follow that this does actually hold for any given PEG matching that language, nor is it particularly likely to hold for a PEG written in a straightforward style.

Perhaps what we want then is a separate stack for these branch and commit points.
When a possibly-unsafe branch point is entered, we push onto this stack, and when a commit point is reached, we pop from this stack.
Iff the stack is empty, it is safe to emit events.

Looking at ../PEG_ES5_arith.peg for inspiration.
This is a grammar that parses only a subset of ES5 arithmetic expressions, i.e. big enough to be interesting but not as big as the full ES5 grammar.
The start rule is:

Expr ←
  AssignmentExpr (S? "," S? AssignmentExpr)*

Or in sexp form:

 (q AssignmentExpr (r (q (o S ϵ) "," (o S ϵ) AssignmentExpr)))
0  1              2  3  4  5 6  7   8  9 A  B

A fairly common pattern in grammars of this sort, as it so happens...

In this case, 1 is ostensibly unsafe as it is the first subexpression of a q, however, since r cannot fail, 1 is actually safe.
Rule: if the last subexpression of a q cannot fail, the previous subexpression becomes streamable.

Subexpressions 4, 7, and 8 are all unsafe, but we don't care about them anyway since they are (probably) very short in terms of matched input, and in any case have no children.
Subexpr B is safe, since it appears in the last position in both of the q expressions (and the r has no effect on safety).
So we can, completely automatically, derive from this rule that all the subexpressions we care about streaming are safely streamable (in any context where the call to Expr itself is streamable, of course).

AssignmentExpr ←
   LeftHandSideExpr S? AssignmentOperator S? AssignmentExpr
 / ConditionalExpr

Here we don't care to make LeftHandSideExpr streamable, as these are generally short anyway.
The ConditionalExpr call here will be preserve safety as it is just the subexpr of an ordered choice.

ConditionalExpr ←
  LogicalOrExpr (S? "?" S? AssignmentExpr S? ":" S? AssignmentExpr)?

This is a little more sticky.
The LogicalOrExpr will not be streamable, as we do not quite know that the ?-expr cannot fail, once it is translated into a (o ... ϵ) unless there is a rule for bubbling up the "infallibility" property through subexpressions (i.e. that (o ... ϵ) cannot fail because ϵ cannot fail and any (o ... x) is infallible if x is.)

However, the streamability of LogicalOrExpr can be provided by the user as an annotation.

Most of the rest of this file seems not terribly interesting.

Looking at the full ES5 grammar, the most interesting cases for streamability are a file consisting only of a sequence of statements and function declarations, and a file containing a top-level immediately executed anonymous function, which is quite common and encloses all but the first and last line of the program in a function expression inside a function call.
In this case, let us see what would be necessary to let the streamability flag survive down through the grammar to the expressions inside the anonymous function.

Program ←
  (S? (Statement / FunctionDeclaration))* S?

FunctionBody ←
  (S? (Statement / FunctionDeclaration))* S?

FunctionDeclaration ←
  FunctionTok S? Identifier S? "(" S? FormalParameterList? S? ")" S? "{" S? FunctionBody S? "}"

FunctionExpr ←
  FunctionTok S? Identifier? S? "(" S? FormalParameterList? S? ")" S? "{" S? FunctionBody S? "}"

Here, a rule that lets the parser generator know that S? cannot fail would be sufficient to make the Statement and FunctionDeclaration occurrences streamable in the Program and FunctionBody contexts.

In a program of the form (function(){ ... many LOC ... })() the parse tree will be:

Program
 Statement
  ExprStatement
   Expr
    AssignmentExpr
     ConditionalExpr
      LogicalOrExpr
       LogicalAndExpr
        BitwiseXOrExpr
         BitwiseAndExpr
          EqualityExpr
           RelationalExpr
            ShiftExpr
             AdditiveExpr
              MultiplicativeExpr
               UnaryExpr
                PostfixExpr
                 LeftHandSideExpr
                * CallExpr *
                   MemberExpr
                    PrimaryExpr
                     "("
                     Expr ... see below
                     ")"
                   Arguments
   EOS

The parse tree under the innermost Expr above will be:

Expr
 AssignmentExpr
  ConditionalExpr
   LogicalOrExpr
    LogicalAndExpr
     BitwiseXOrExpr
      BitwiseAndExpr
       EqualityExpr
        RelationalExpr
         ShiftExpr
          AdditiveExpr
           MultiplicativeExpr
            UnaryExpr
             PostfixExpr
              LeftHandSideExpr
               NewExpr
                MemberExpr
                 FunctionExpr

It is impossible to stream the meat of this program, the inner FunctionExpr, given this grammar.
The reason is that, if the last "()" were not present, the CallExpr, marked in the tree above with asterisks, would not appear in the tree, and would be replaced with a NewExpr instead.
This is because of the way the ES5 grammar is structured.
In the grammar a CallExpr requires MemberExpr followed by at least one Arguments, even though if the arguments are not present, the LeftHandSideExpr will still parse, but it will do so by falling back to NewExpr, which does not require the trailing arguments after the MemberExpr.
To make this grammar streamable, then, we need to rework LeftHandSideExpr so that it parses the same language, gives equally useful trees, but doesn't backtrack over quite so much of the text.

Interestingly, one of the great benefits of PEGs is that they allow cheap backtracking, in this case, backtracking over an entire file without any extra reparsing cost, as the MemberExpr successful parse result matched as part of either the CallExpr or NewExpr will already be cached.
Unfortunately the reliance on cheap backtracking is not compatible with streamability.
Backtracking is cheap in a packrat parser, but it makes streaming impossible in any expression that may be successfully backtracked over.

In LR parsers, the emphasis is on committing to a parse tree early.
This is why the parse tree above contains such deeply nested rules, (Expr → AssignmentExpr → ConditionalExpr → …)
If backtracking were cheap we could have AssignmentExpr require an actual assignment, and first try the AssignmentExpr, and then, if that fails, try the ConditionalExpr at the same position, and then if that fails try the LogicalOrExpr, and so on.
In this way we would get a much smaller tree, and exclude such unintuitive parses as an AssignmentExpr that doesn't actually contain an assignment operator or a RHS.
The cost is that it is impossible to know until the end of the expression is reached which of these cases will actually succeed.
Automatic rule elision of course suffers from the same weakness: until the AssignmentExpr is closed, we cannot know whether it will subsume the ConditionalExpr as its co-extensive only child node, or whether it will be a "true" AssignmentExpr.

Given that the ES5 grammar is generally very solidly in the LR tradition, then, why did this late-choice division between CallExpr and NewExpr arise?
This is one of the ugliest parts of the grammar.

Taking the CFG productions from ES5:

LeftHandSideExpression :
      NewExpression
      CallExpression

NewExpression :
     MemberExpression
     new NewExpression

CallExpression :
      MemberExpression Arguments
      CallExpression Arguments
      CallExpression [ Expression ]
      CallExpression . IdentifierName

MemberExpression :
     PrimaryExpression
     FunctionExpression
     MemberExpression [ Expression ]
     MemberExpression . IdentifierName
     new MemberExpression Arguments

A LHSExpr can be either a NewExpr or CallExpr.
A CallExpr is a MemberExpr followed by one Arguments, followed by any number of Arguments, "[" Expr "]", or "." IdentifierName.
A NewExpr is "new"* MemberExpr.
A MemberExpr, confusingly enough, can match "new" MemberExpr Arguments, but not "new" MemberExpr without arguments, and can also produce any number of "[" Expr "]" and "." IndentifierName suffixes.
Ultimately a MemberExpr must resolve to either a PrimaryExpr or FunctionExpr; these include self-contained things like literals, "this", identifiers, and parenthesized expressions.

So ultimately, a LeftHandSide expression contains a PrimaryExpr or FunctionExpr, surrounded by some optional prefixed "new" and optional suffixed Arguments, and bracket or dot property accessors.
It is possible for a NewExpr to not actually match the "new" token, in fact this is probably the common case, when the occurrences of "new" and Arguments are balanced.
If the "new" and Arguments are balanced (possibly with property accessors inside), this is a MemberExpr.
Examples:

a
new a.b['c'].d()
new new a()()

If the "new" occurrences and Arguments occurences are unbalanced in favor of arguments, this is a CallExpr.
These can have any number of trailing property accessors also.
Examples:

a()
new a.b['c'].d()()
new new a()()()
new new a()()()[b].c.d

In the middle of the PrimitiveExpr or FunctionExpr at the heart of one of these LHSExprs, the number of leading 'new' tokens is known but the number and order of trailing property accessors and arguments is not known.
For the purposes of an interpreter the distinction is significant, but an interpreter would need to wait for the entire parse tree anyway.
For the purposes of a syntax highlighter, or code completion assistance, or in incrementally reparsing the contents of an inner FunctionExpr, the distinction between a surrounding CallExpr or NewExpr is not as significant.

The language matched by LeftHandSideExpr is simply "new"* (PrimaryExpr / FunctionExpr) (DotAccessor / BracketAccessor / Arguments)*.

This can be streamed if LeftHandSideExpr is replaced with this rule.
The difference in the generated parse tree is that, instead of nested expressions reflecting the language's semantic structure, any expression that matches LHSExpr will have the same form.
The distinctions in semantics will then have to be sussed out by looking at the trail end of property accessors and Arguments, matching them up to any 'new' tokens seen at the front, to evaluate the structure semantically.
This could be done after the fact in a fixing-up step which would replace the streamable tree with a tree that captures the language semantics more directly.
For example an IDE might want to use the streamable representation for syntax highlighting, but the more semantic structure for code completion, type inference, etc.
The ES5 specification uses the grammar productions directly in defining the evaluation of expressions.
In particular, each production above the level of the lexical grammar has a fixed number of children, so there is no looping in the definitions of the evaluation algorithms.
In the case of the streamable tree, the same semantics can of course be defined, but it requires dealing with a list of suffixes one at a time.

This streamable grammar variant is now implemented in ../ECMAScript_streamable.peg.

With the streamable grammar, the parse tree of (function(){ ... })() will now be:

Program
 Statement
  ExprStatement
   Expr
    AssignmentExpr
     ConditionalExpr
      LogicalOrExpr
       LogicalAndExpr
        BitwiseXOrExpr
         BitwiseAndExpr
          EqualityExpr
           RelationalExpr
            ShiftExpr
             AdditiveExpr
              MultiplicativeExpr
               UnaryExpr
                PostfixExpr
                 LeftHandSideExpr
                  PrimaryExpr
                   "("
                   Expr ... see below
                   ")"
                  Arguments
   EOS

The parse tree under the innermost Expr above will now be:

Expr
 AssignmentExpr
  ConditionalExpr
   LogicalOrExpr
    LogicalAndExpr
     BitwiseXOrExpr
      BitwiseAndExpr
       EqualityExpr
        RelationalExpr
         ShiftExpr
          AdditiveExpr
           MultiplicativeExpr
            UnaryExpr
             PostfixExpr
              LeftHandSideExpr
               FunctionExpr

Note that MemberExpr, CallExpr, and NewExpr have all been eliminated.
LeftHandSideExpr remains and matches the same language but with different parse trees.

Now that our concept of the stack is sufficiently well developed, we can define the streaming behavior in terms of the stack contents.
Each expression in the grammar (each of which corresponds to a state) is either 'streamable' or 'atomic', 'unitary', 'unstreamable'.
Then, if the stack contains any unstreamable state, we are in non-streaming mode, otherwise we are in streaming mode.
So streaming mode can be maintained as a flag which is set or unset as we push and pop the stack.
When in streaming mode we can emit a tree chunk at any time, including on every rule match, which may be convenient for debugging at least.
Otherwise we must buffer until non-streaming mode is entered again.
The v5 parser behavior is just a special case in which the start rule is non-streamable.

To verify that streaming mode "works" on a corpus we could record a position and move it forward at every expression we enter in streaming mode, and then at any occasion where we backtrack, throw an exception if the position we backtrack to is earlier than this maximal point at which streaming could have happened.


At least in a debugging mode, when we enter a rule we should record a distinguished value in the result table.
If that value is seen then we have left recursion or something equivalent to it.
Ideally this would be detected at compile time, which is why the runtime check should not need to be added, but the latter is much easier to implement.


For simplicity in the code generator we also replace all string literals with q( "a" "b" ... ).
Thus the only terminal test is (possibly singleton) cset membership.
Later the optimization can be added back in of testing multiple characters without a state change and a loop.
Incidentally the grammar can be analyzed to find overlapping sequences that may be unified into a single such state (this simply compresses the code for such things as "\r\n|\r|\n" which may appear directly in multiple places in a grammar).

making each character test only consume a single character makes it possible to compose character tests.
For example there can be a faster ASCII test for characters in a range, and then if this fails the state can simply be switched to a state that does more expensive comparisons to handle a wider range of characters.
If two character ranges are used in the same parser and they are both very large and one is a superset of the other, it can be implemented as the other with the difference as the error fallback, etc.


What we probably really want is not to say which rules should be chunked, but rather which rules (and maybe which parts of that rule's RHS) should be entered into and returned partially when chunking.
In this case there would be no need to mention the atomic rules, that is, any rule which will never have any child nodes, such as Num.
However we would want to mention the top-level rule if there is to be any chunking at all.
We would also want to mention here that Add and Mult can be entered and returned partially.
Another way to do it perhaps would be to exclude certain expressions (not just rules) from chunking.

One thing we could also possibly do with these annotations is say "this annotation is an assertion that if this expression succeeds, the parser will never backtrack back over this point (unless it fails)" and in this case an actual assertion could be added to the generated code and then run against a test corpus to heuristically test the premise.

// the only case this might matter is if someone tries to resume a failed parse but then pass the original tbl reference to error analysis.
// I guess we can reset tbl at the point when we return it.

unrelated note:
interestingly the same time-annotated tree structure used by revstore can serve as a representation of a time-varying parse tree
rather than having a single value, each node in the tree has a sequence of (time, new value) pairs which describe its history


Rewriting this by hand to be streaming, first we must change the API.
The parser generator gets a list of 'chunk' rule names along with the grammar.
In this case the grammar never backtracks over any of these rules unless the entire parse fails, so each of them can be a chunk point.

The API of the generated parser is a little different.
Rather than returning a parse tree it returns a function which takes a chunk of input as argument and returns a [

Several API ideas:

- parser returns a function, which takes a chunk and returns a parse tree fragment.
  the function is a closure and maintains its own state.
  the caller keeps feeding chunks to the same function.

- parser takes a chunk of input and an optional state structure.
  it returns a parse tree fragment (possibly empty of course) and a state structure (which is opaque to the caller).
  all state is in the state structure, so multiple streams can be parsed at once, resumed on different input, etc.

- parser takes a function `out` and returns a function `in`.
  `in` is then called repeatedly with each chunk of input.
  when `in` is called, it will call `out` with each parse tree fragment.

Since there is no way to copy a closure with its internal state, maintaining state only in a closure is probably less desirable.
Of course we could add a 'copy' message that can be sent to the function which will clone the internal state and return a new closure.

The ability to inspect the returned state might be useful for debugging, but this would require some helper function to interpret the state, and a debug message could just as easily be sent to the function for this as well.

This latter idea is nice, I think it will make a good example of OO done in a JavaScript style.
Messages can be passed as the first argument to the function as a string, followed by any arguments.
Another advantage of the last idea is that it can be made asynchronous (e.g. by passing the actual parsing off to a worker) without any API change.

Parsing strategy
----------------

When entering a rule, there can now be three possible results: success, failure, or premature end of input.
After the last chunk the caller must signal it by sending an EOF message.
This message converts the premature end of input state into a final parse result.
Whenever the end of the input is reached, this state needs to be entered, and the parse needs to resume on the next chunk at the first rule that went past the end of the previous chunk.


unrelated idea:

There is a program transformation from any ECMAScript program to a new program which makes no function calls.
The general strategy is: for each function, define a unique state value and a set of local variables.
Structure the program as a loop which maintains its own stack, tests the state on each loop entry, and contains corresponding function bodies in a switch statement on the state value.
It is obvious that this works, since it is exactly what an interpreter does, we are simply hoisting part of the interpreter into the program.
However, it has some interesting side effects.
One is that tail call elimination and such features can be implemented by such a compiler.
The other is that, since we have full access to the stack, we can implement call/cc.
If all data are serializable we can even at any time serialize the entire program state and resume it later at will.
The transformation from the mutually-recursive-function v5 parser implementation to the streamable v6 implementation is essentially just this transformation applied to the original parser program, with a few optimizations applied to the result (e.g. eliminating local variables in some states, and combining some state values) based on domain-specific expectations about how the program will execute.