In reasoning about a PEG, we can construct hypotheses about the relationships between expressions in the grammar, e.g. the reachability of certain parser states from certain other states.

Formalizing these hypotheses gives us a way to construct reasoners that can prove certain characteristics, e.g. to prove the correctness of a particular transformation to a more efficient parser for a particular grammar.

There are a class of optimizations that are valid only in the context of a particular complete grammar, e.g. to generate a parser as efficient as an LR(1) parser with a separate tokenizer, we first have to recognize that the language expressed can be correctly parsed by the reduced power of an LR parser.

What is desired is a framework in which we can put forward hypotheses about expressions in a grammar, and validate them with proofs, or disconfirm them with counterexamples, whereby such reductions in parsing power can be justified and more efficient parsers generated automatically.

The earlier work on retry sets made use of ad-hoc hypotheses about possible past and future states of a recursive descent parser, based on a hypothesized position result, namely the failure of a particular occurrence of a nonterminal.
By completing this set of possible future states, the algorithm can determine whether a rule failure allows for the possibility that the same rule will be tried again at the same position.
If so, the result may be cached in the result table, but if not, there is no value in storing a result that will never be used.

Initially we attempted to prove the unreachability of positions by analyzing the offsets into the input that each expression allows.
However, a quick look at the ECMAScript 5 grammar shows that there are many other computable aspects of a parser state hypothesis that can be used to disprove reachability.
The most obvious is disjoint expressions, e.g. two PEG expressions which can be proved to never match the same input, such that if one succeeds the other will never succeed at the same input position.
Then if we have, e.g.

S ← A / B
A ← 'a' C
B ← 'b' C
C ← ...

Given that C appears nowhere but A and B, we can easily prove that if A ← 'a' · C is reached and fails, C will never be tested again at that same input position.

But we can make a much more radical optimization than simply not caching the failure result of C here.
Instead, we can recognize that this grammar will only match inputs beginning with 'a' or 'b', and on that basis we can place the result of the entire parse in one of three categories, based on a single test of the first input character, as a bottom-up parser would do.
If the first character is 'a', the parse will either match (S (A ('a' C))) or it will fail, and similarly for 'b'.
Otherwise the parse has already failed.
A recursive descent or packrat parser will first descend into the A rule, then test 'a', then, if that fails, descend into B and test for 'b', and then if that fails, see that there are no further alternatives and fail the entire parse.

We can construct such a proof algorithmically as follows:
By hypothesis, we have the position result (A ← 'a' · C, false, X), indicating that the rule C has been tested at some position X and has failed.
There are only two uses of C in the grammar.
The occurrence in A will not reached again at the same input, since the only way for that to happen would be if A was tested again at the same input position, and if that were possible, the A result itself would have been cached.
(This is not always the case however, so even this must be proved, e.g. in a rule A' ← 'a'* C, if the rule is tried repeatedly at different positions on input which contains a string of 'a's, then C may be tested repeatedly at the same position, even if this is the only occurrence of C in the entire grammar.)
Then we need consider only whether the position B ← 'b' · C can be reached at X.

In the hypothesized position result, we have entered a sequence, so we can prove that the earlier part of the sequence must have succeeded on an earlier part of the input.
We generate, from the 'a' expression, a position offset of 1, and subtract this from X to generate a new position result (A ← · 'a' C, true, X-1), which is known to have occurred.

Then, since we wish to test whether B ← 'b' · C can be reached at X, we hypothesize that it has been reached.
From this, we can generate by a similar process the result (B ← · 'b' C), true, X-1), which must have occurred.
We can then show that, because 'a' and 'b' are disjoint, (· 'a', true, X-1) and (· 'b', true, X-1) are incompatible.
Since we have reached a contradiction, the second hypothesis must be false, and we have proved that C cannot be reached again.

So in general, we begin with a nonterminal occurrence, which we assume has failed, and we wish to determine whether the failure result can be reused if it is cached.
We then find some other occurrence of the nonterminal in the grammar (either in the same rule or in another rule).
For each such occurrence, we hypothesize that that position is reached at the same input position, i.e. that the position result (... · α ..., false, X) is followed later in the execution of a parser by the position (... · α ..., X), and attempt to reach a contradiction from these hypotheses.
If we can prove that these hypotheses are incompatible for every occurrence of the nonterminal, we need not cache the failure result.

To compute these, then, we need some way to represent hypotheses and propositions, a way to derive new propositions from old, an algorithm that guides the process to a result, and a way to recognize that a proof has been reached.
In other words, a bug-ridden, slow implementation of half of Prolog ought to do it.

So, given a use P of rule A, we first find all occurrences in the grammar of A.

A parser position is a pair N × ℕ of a nonterminal in the grammar and a position within the corresponding rule.
Parser positions are typically written by writing the rule, or a part of the rule, a middle dot before the nth sub-expression of the RHS to indicate the nth position of the rule.
We define a parser step (or simply step) as a tuple ℕ × N × ℕ × ℕ of a step index, a parser rule, position within the rule, and an input position.

A parser proceeds by transitioning from one step to another step or to a parse result.
A parser may transition to a new step on the basis of the previous step's rule position and the symbol at the input position.
Parser optimizations may replace any sequence of transitions with any other computation that produces the same result, e.g. a packrat parser looking up memoized results in a result table rather than re-parsing input.

A parser step result (or step result) is a parser step with a Boolean value and an input position offset.

Unfortunately, the question of disjointness of two parser expressions in the general case may be undecidable.

--------------
worked example
--------------

In the ES5 PEG, we have the following rule:

DoWhileStatement ←
  DoTok S? Statement S? WhileTok S? "(" S? Expr S? ")" EOS

We wish to write a program which can produce a proof that, if the position result

  ( DoTok S? Statement S? · WhileTok , false )

is reached by a parser, then the WhileTok failure result will not be used again and need not be cached.

It so happens there is one other occurrence of WhileTok in the grammar:

WhileStatement ←
  WhileTok S? "(" S? Expr S? ")" S? Statement

Both of DoWhileStatement and WhileStatement appear only in:

IterationStatement ←
   DoWhileStatement
 / WhileStatement
 / ForInStatement
 / ForStatement

and the only occurrence of IterationStatement is in:

Statement
  ← Block
  / VariableStatement
  / EmptyStatement
  / ExprStatement
  / IfStatement
  / IterationStatement
  / ContinueStatement
  / BreakStatement
  / ReturnStatement
  / WithStatement
  / LabelledStatement
  / SwitchStatement
  / ThrowStatement
  / TryStatement
  / DebuggerStatement

Statement in turn appears in many places in the grammar.

This proof is extremely difficult to do even by hand, in fact, because it relies on a lot of implicit assumptions about how parts of the grammar relate.
Because Statement appears frequently, and appears within Block (between brackets) which also appears in many places, it is quite hard to prove that, if a DoWhileStatement fails, there will not be some outer rule that will then retry WhileTok at the same input position.
In fact I am not even confident that this is the case, there may be cases where a WhileTok will be tried there again, though if it is I am sure the entire parse will fail.