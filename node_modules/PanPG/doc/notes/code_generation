These are notes taken during the writing of the various code generators (of which there have now been six).
Mostly these are of historical interest.

-----

If we have a production which consists only of a character class, we can produce an ECMAScript regex to match it.

Example: The rule "Letter ← [ a-z ]" becomes something like "p_Letter(...){...re(/[^a-z]/)...}"

We have csets which allow set operations on sets of characters, so "[ a-z − x-z ]" matches a-w.

Currently this does not apply to merging productions which include only character sets.

Example:

  Letter ← [ a-z ]
  Digit ← [ 0-9 ]
  LetterDigit ← Letter / Digit
  LDString ← LetterDigit+

We currently create a function call for each production, and unite rules such as "/" ordered choices with combinator functions, so we would have a matcher like:

  p_LDString(...){... rep(1,0,p_LetterDigit) ...}

  p_LetterDigit(...){... ordChoice(p_Letter,p_Digit) ...}

  p_Letter(...){... re(/^[a-z]/) ...}

  p_Digit(...){... re(/^[0-9]/) ...}

This same language could equally well be matched by:

  p_LDString(...){... re(/^[a-z0-9]/) ...}

Which we can expect to be a lot faster.

Of course, the second version will not generate the same parse tree, so this can only be done when the elided rules will not be accessed in the tree that results.
The set of rules that are actually used can be pulled from a tree mutation program, and the other rules can then be avoided.
An interactive, incremental parser which allows real-time exploration of a parse tree could also reparse fragments only when the user descends into the part of the tree that requires them.

A general-purpose JavaScript optimizer could help with some of this by doing inlining, etc, but would not likely combine regexes, and would not be able to remove the code that does tree construction in the elided rules, so we must do this before JavaScript is generated.

Where this potentially applies:

An OrdChoice production may combine its Sequence children into a union regex.
This loses the distinction between the alternatives.

A Sequence may combine its children.
This loses the ability to access those children in the parse tree.

Any of the four repetition productions may be combined into a regex rather than a call to rep().
This removes its children from the parse tree.

Positive and negative lookaheads have no children in the parse tree anyway, so they can always be merged with any subsequent sibling of a Sequence.

Negative lookaheads, when both the lookahead and what follows are a charset, may be merged in by simple set difference.

The Empty production can be optimized away in many cases by altering an U[OrdChoice] D sibling.

String literals and character sets already do not contribute nodes to the parse tree so they may always be optimized away where convenient.

*** Performance considerations ***

Regexes may have different performance characteristics wrt to backtracking.

More significantly, if mutually recursive productions are elided, the linear performance PEG guarantee may be lost.

Consider:

  S ← a S* b / a S* c / a S*

on input: 'aaaaaaaa' (eight 'a's).

Ordinarily, the initial descent will match each 'a' against the first branch.
The ninth call to S will fail (on all branches) at the EOF, so the eighth will try the second branch.
At this point the 'a' matches, and the result of S is already known (fail), so the 'c' is tried, and fails.
Then the third branch is tried, and succeeds (again using the cached S result).

We then have a cached result for S at position 8 (failure) and 7 (success).
We now are back in the seventh call to S, in the first branch, and try to match a 'b', which fails.
The second branch is tried, uses the cached S result at position 7, and fails.
The third branch succeeds, so the seventh S call succeeds, and we have a new cached S success result at position 6.

In this way the success bubbles back towards the front of the string, without any unnecessary recursion.
Each branch will still be tried at each position, but only once.
If this is compiled into a regex... hm.

I seem to have picked an example that's inexpressible as a regex.



On Mar 20, 2009, at 3:19 PM, Tim Goodwin wrote:
> > So my question still stands: can anybody show me a PEG that - for a
> > backtracking recursive descent parser - would be more complex than
> > O(n) to parse? Despite all the discussion of exponential complexity in
> > this post, that's not necessary: a mere O(n^2) would be great!

Try:

S <- A*
A <- a S b / a S c / a

on a string consisting of all a's.  A little contrived admittedly. :)

Bryan

_______________________________________________
PEG mailing list
PEG@lists.csail.mit.edu
https://lists.csail.mit.edu/mailman/listinfo/peg



With the above PEG, on input "aaaaaaaa".

Call the alternative branches of A A0, A1, and A2 respectively.

S tries to match an A.
A matches the first 'a' to the first alternative, and tries to match an S.

The stack so far:

S A S

S then tries to match an A, which now matches the second 'a' in the input, and so on to the end of the string.

When the pointer first gets to the end of the input string, the stack looks like:

S A0 S A0 S A0 S A0 S A0 S A0 S A0 S A0 S A

The eighth A has matched the last 'a' in the string.
The ninth A tries all three branches but they all fail because there is no more input.
The ninth S then succeeds, matching nothing.
The eighth A then tries to match a 'b' and fails.
The eighth A then tries to match A1, the 'a' and S match, but the 'c' fails.
The eighth A then matches A2 to the last A in the string and succeeds.
Control then returns to the eighth S, which tries to match another A (which fails), and then succeeds, having matched one A.
Control returns to the seventh A0, which then tries to match a 'b', which fails, because there is no more input, the last 'a' having been consumed by the S.
The seventh A then tries A1, which matches the seventh 'a', and tries S again.
This new S will again succeed, exactly as the eighth S did, and the A1 will then fail since there is no 'c'.
Finally the seventh A will succeed, matching the seventh 'a', and control will then return to the seventh S...

So the failures bubble backwards from the end of the string in a fractal pattern towards the beginning.
The entire chain since the first entry into the A0 branch at the first input character is doomed.

Now, assume memoization.

The process will proceed as above until the first time the end of the string is reached.

The stack again is:

S A0 S A0 S A0 S A0 S A0 S A0 S A0 S A0 S A

The eighth A has matched the last 'a' in the input against its A0 branch.
It tries to match the ninth S which succeeds (as the ninth A fails), and then looks for 'b' and fails.
It then tries A1, which again matches the last 'a', and then tries to match an S.
Here we have the first use of a cached result; the S is known to match without any further recursion.
The 'c' then fails, so the A1 fails, so the A2 is tried, and succeeds.

We then are back in the eighth S, which tries to match another A, which again is a cached result.
The eighth S succeeds, having matched one A.

The seventh A now regains control.
It is still in the A0 branch, and the S has succeeded, so it tries the 'b', which fails.
Then in A1 it matches the 'a' again and matches the S again (cached), and fails on 'c'.
Then in A2 it matches the 'a' alone and succeeds.

The seventh S now regains control, it's first A having succeeded, consuming one character.
It tries to match another A at position 8.
This is a cached success result.
The seventh S succeeds.

Thus at each position, each branch of the A will be tried.
S will be tried at each position until the first time it succeeds.

Interestingly, the successes will also bubble back up, once S has matched at position 7 it will next match at position 6, then it will eventually match at position 0 and terminate.




------------
Rule Elision
------------

We start with a PEG and a set of rule names that will not be accessed directly in the parse tree.

We generate both re objects (described below) and parse code in a single TMProg.
Both the re objects and the parser code may contain references to other rules (the parser code in the form of references to other parser functions, the re objects as described below).

Once re objects and parser code is generated, we have a table on the root node which contains an re object and a parser template for each rule.
A parser template is a function from an re object table to a JavaScript parser.

The caller can remove re objects from the table on the basis of intended use, and then provide (a subset of) the table to the parser templates to generate JavaScript parser code which retains the desired level of detail in the generated parse tree.


-------------------
The re object table
-------------------

Re objects represent a potential regex which may contain named references to other re objects which must be provided for the regex to be constructed.
If the transitive closure of an re object contains a cycle, then no regex can be constructed.
Otherwise, the regex object is constructed by substitution, though the PEG performance guarantees may not be preserved in all cases.


------------------------
re table post-processing
------------------------

input: a table of re objects, each identified by a name (typically the name of the parser rule).
output: for each name in the input table, either a self-contained re object, or an explanation of why there isn't one.

state: we maintain a parent stack, an index into the input table, and the (partial) output table

-  create an empty parent stack and output table
1
-  let A be the next item from the input table, as described below.
-  if A is null, there are no unprocessed elements.
   -  return the output table.
-  otherwise, A is the next unprocessed input element:
   -  push A onto the parent stack
2
*  invariant: at this point the variable A is set, and is the item at the top of the parent stack.
-  if A is self-contained as described below:
   -  insert the self-contained object in the output table
   -  pop A from the parent stack.
   -  if the parent stack is empty:
      -  continue from 1 above.
   -  otherwise:
      -  let A be the element at the top of the parent stack
      -  continue from 2 above.
-  otherwise, let D be the found dependency of A.
   -  if D exists in the parent stack:
      -  for each item in the parent stack, add a cycle error to the output table.
      -  empty the parent stack
      -  continue from 1 above.
   -  if a successful re object for D is already in the output table:
      -  replace the occurrence of D in A with the value from the output table.
      -  continue from 2 above.
   -  if an error entry for D is in the output table:
      -  for each item in the parent stack, add a dependency error to the output table.
      -  empty the parent stack.
      -  continue from 1 above.
   -  otherwise, D does not exist in the output table.
      -  push D onto the parent stack.
      -  let A have the value of D.
      -  continue from 2 above.

finding the next item from the input table:
-  if the current index is at the end of the input table, return nothing
-  let B be the element of the input table at the current index
-  if B exists in the output table, increment the index and try again, otherwise, return B

determining if an re object is self-contained, and finding the first dependency otherwise:
-  if the re object is an atomic object such as a string literal or character set, it is self-contained.
-  otherwise the re object is a composite; it is self-contained iff all of its components are self-contained.
-  if the object is not self-contained it has a dependency, which we return.



InputElementRegExp is *huge*



----------------
re optimizations
----------------

There are six types of re objects: cset, strlit, sequence, union, repetition, named reference, and negation.
Named references must be replaced before an re may be serialized, so only the other five concern us.

all can be serialized straightforwardly
some combinations can be optimized

A sequence can always be serialized directly, by serializing its parts and simply concatenating them, with parentheses added when necessary.
A sequence of a negative forward lookahead of a cset N followed by a cset P may be optimized as the cset_difference of P minus N.

A union can always be serialized by intercalating the serialization of its parts with "|", parenthesizing when necessary.
A union of csets can be serialized more efficiently by serializing the cset union.
Adjacent cset res (in union order) may be unified even if there are other members of the union, but, in general, non-adjacent may not.
However if there is no overlap between the cset and any other member of the union, they may be reordered freely.
A union of many strlits may be optimized in some cases by combining prefixes, e.g. foobar|foobaz → fooba[rz].
Or suffixes.
A union of repetitions which differ only in number could be unified into a single repetition, though this case seems unlikely to arise in practice.
A union of negations may be converted to a negation of a union, which might have benefit if that union can then be further optimized.

A repetition of a repetition, though probably rare, can be optimized by multiplication.
A repetition of a positive or negative lookahead assertion is equivalent to that assertion.

A negation of a cset may be converted to a cset.
A negation of a strlit may be converted to several equivalent forms, but they are unlikely to be useful.

The current problem is that the serialized re output for the ES5 Identifier production is so unwieldy as to be unusable in practice.

An Identifier is an IdentifierStart character followed by any number of IdentifierPart characters, so long as the entire token is not a reserved word.
IdentifierStart and IdentifierPart are huge csets for large swathes of Unicode codespace as well as escapes of the "\uHHHH" form.
Reserved words are a small set of about 64 words such as "for" and "in".
Any reserved word may appear as a prefix of an identifier, so long as it is followed by other IdentifierPart characters.

To write this as a regex (using "IDS" and "IDP" as shorthand for the relevant character classes) we might write:

(?!(?:for|in|...)(?![IDP]))[IDS][IDP]*

That is, if a reserved word not followed by a character in IDP matches, then fail, otherwise match an IdentifierStart followed by any number of IdentifierPart characters.

The PEG already includes rules ending in -Tok for many of the reserved words, since these are used elsewhere, and looks something like this:

Identifier ← !(ReservedWord) IdentifierName
IdentifierName ← IdentifierStart IdentifierPart*
IdentifierStart ← …
IdentifierPart ← …
ReservedWord ← ForTok / InTok / ...
ForTok ← "for" !(IdentifierPart)
InTok ← "in" !(IdentifierPart)
…more tokens

Code generation is not smart enough yet to get the desired regex from that PEG.
What is required is:
- moving the expensive !(IdentifierPart) suffix to the end of the ReservedWord choice
  - there must be a rule that can recognize that A B / C B / D B is equal to ( A / C / D ) B.
  - there may need to be some cost annotations on the re structure so that the latter can be identified as clearly cheaper (though since it is strictly shorter this may not be required)
  - the re structure must be preserved long enough for this optimization to take place
Currently none of these three requirements are met.

Currently the generated re for IdentifierPart, based on the PEG rule,

IdentifierPart ← IdentifierStart / ...

includes the entire regex for IdentifierPart, followed by the entire regex for another large character class.
The re simplification code is not smart enough to combine these, because the former includes not just a cset but also matches \uHHHH escapes.
Adding the necessary rule so that this gets simplified would shorten the Identifier regex by roughly half.



---------------
code generation
---------------

Code generation takes a start token, an re table, a parser function table, a dependency table, and a set of rule names either to elide or to preserve.
Output is a string which is an ECMAScript parser for the start token which generates a parse tree containing at least those rule names which have not been elided, and possibly others.

Building the dependency table:

The dependency table contains, for each parse rule, the parse rules, if any, on which it immediately depends, i.e. its children in the dependency graph.
We produce the dependency table in the main PEG codegen TMProg.

Each rule in the dependency tree may be output as a an re-table-based parse function, a parser function from the parser table, or it may not contribute to the output at all.

We call re-table-based parse functions self-contained, and parser-table functions recursive.

For each recursive function in the output, its dependencies must also appear in the output.

Every dependency in the graph rooted at the input rule name which is not in the elision table must appear in the output.

So we must walk the entire dependency tree, checking for those already seen to avoid loops, and test each against the elision table.

This gives a partial set of rules which must appear in the output.

Every ancestor of any member of this set must appear as a recursive function.

Now we have a set of functions which must appear as recursive functions to generate the required parse tree.

We also have a set of required functions which must appear, but can be self-contained.

These sets can be determined in a single traversal of the dependency graph.

Of the functions which must appear but may be self-contained, that is, those which are not elidable but do not have any non-elidable descendants, some may also need to be output as recursive functions because of dependency cycles in the graph.
These will be rules for which no entry exists in the re table.

If a rule is output as a self-contained function, this means that it has no un-elidable descendants and that it appears in the re table.
In this case none of its descendants need to appear in the output.
We cannot know whether this is the case until the descendant tree has be traversed, but once it has, we do know, as any other part of the graph which depends on this node cannot prohibit it from being self-contained.

Each rule can be characterized as either elidable or not.
This depends on the rule and all its descendants being in the elision set.

Each rule can also be characterized as required or not.
This depends on whether it is elidable, and on whether any of its immediate parents in the dependency graph are recursive.

Each rule can also be characterized as self-contained or recursive.
This depends on its descendants being elidable, and on the re table.

We call the following algorithm with the start rule as input, discard the optional flag and return the output set of the result.

-  if the rule appears in the result table, return the result
-  unset the appears and recursive flags
-  if the rule is not elidable, set the appears flag
-  add the dummy entry (required, {}) to the result table
-  for each dependency of this rule, run this algorithm
   -  if the return value is (required, x)
      -  set the appears flag
      -  set the recursive flag
      -  add x to the output set
   -  if the return value is (optional, x)
      -  add x to the optional output set
-  remove the dummy entry from the result table
-  if the recursive flag is not set and this rule does not appear in the re table
   -  set the recursive flag
-  if the recursive flag is set
   -  add the optional output set to the output set
   -  look up this rule in the parser function table and add the value to the output set
-  otherwise, the recursive flag is not set
   -  look up this rule in the re table, and add it to the output set
-  if the appears flag is set
   -  let the return value be (required, output set)
-  otherwise
   -  let the return value be (optional, output set)
-  add the return value to the result table
-  return the return value

An output set is a set of rule names, and for each, a flag set to 1 or 0.
If the flag is set to 1, we add the recursive parser function to the output, if 0 we add the self-contained re.
Adding one set to another is a simple set union operation.



----------
codegen v3
----------

one function
state table
string
current position
re objects in scope

Instead of function calls, we have a switch statement and a loop.
Most case statements test a regex, and if it succeeds, the current position and the state is updated.
No, too complicated for now.

we still use functions for recursion, the only difference is that re objects and others are shared in the outer function scope so it should be more efficient, and we use re.lastIndex instead of creating new strings.



----------
codegen v5
----------

remove all extraneous function calls

2 * 3 + 4

Expr
 Add
  Mult
   Num 2
   Num 3
  Mult
   Num 4

Expr ← Add
Add ← Mult ('+' Mult)*
Mult ← Num ('*' Num)*
Num ← 0-9+

For a cleaner parse tree we might write:

Expr ← Add / Mult / Num
Add ← ((Mult / Num) '+')+ (Mult / Num)
Mult ← (Num '*')+ Num
Num ← 0-9+

In LR(0):

(0) Expr → Add
(1) Expr → Mult
(2) Expr → Num
(3) Add → Expr '+' Expr
(4) Mult → 

LR(0) uses reductions rather than recursive descent, so in parsing the above, assuming a tokenizer:

State  |      Token      |           GOTO
-------|-----------------|-------------------------
       | Num |  *  |  +  |  Expr | Add | Mult | Num
       |-----------------|-------------------------
0      | s1  |                                | 2
1      | r3


We begin in state 0.
The 2 is a Num token, and this and the current state tells us to shift to a new state, 1, and also puts the Num on the stack.
In state 1, we reduce, popping the state 1 off the stack and outputting rule 3 with the Num token (having value "2").
Having popped the stack we are now in state 0 again.
However, when we reduce we look up the state and the reduced rule in the GOTO table and push the new state.

-----------

Expr ← Add / Mult / Num
Add ← ((Mult / Num) '+')+ (Mult / Num)
Mult ← (Num '*')+ Num
Num ← 0-9+

2 * 3 + 4

disregarding whitespace:
2*3+4

(Expr (Add (Mult (Num 2) (Num 3)) (Num 4)))

Expr at 0
 Add at 0
  +-quantifier, try 1
   Mult at 0
    +-quantifier, try 1
     Num at 0
      consumes '2'
     consumes '*'
    +-quantifier, try 2
     Num at 2
      consumes '3'
     fails on '+' expecting '*'
    the +-quantifier succeeded once, which is enough, so Mult at 0 continues
    the position is rolled back to 2
    Num at 2
     cached result, pos now 3
    this is the end of the Mult rule so it succeeds, corresponding to a reduce action in an LR parser
   back in the add at 0 rule, pos is 3 and Mult has succeeded
   consumes '+' at 3, pos←4
  +-expression, try 2
   Mult at 4
    +-expr, try 1
     Num at 4
      consumes '4', pos←5
     fails at 5 expecting '*' but seeing EOF
    since +-expr failed, Mult at 4 fails, pos rolls back to 4
   Num at 4
    cached succees, pos←5
   fails at 5 expecting '+', so the +-expr try 2 fails and pos rolls back to 4
  Mult at 4
   cached failure
  Num at 4
   cached success, pos←5
  this is the end of the Add rule so Add 0 succeeds
 this is the end of the Expr rule since Add 0 succeeded, so Expr succeeds and we are done

Rather than recursive descent through function calls, we can directly manipulate a stack.
The first effect the input will have on the stack is in Expr → Add → Mult → Num, when the first character either matches 0-9 or does not.
We find this position by simply taking the first alternative of every branch from the start symbol until the first terminal.
Rather than beginning with an Expr call, we actually initialize the stack to be in this state and begin by testing the first input symbol.
At this point we have an Expr, Add, Mult, and Num on the stack at the same position, 0.
This is also the position at which we are testing the input symbol.
Some of these results will need to be cached, but for some we can prove that the cached result would never be used.

Borrowing the LR /item/ terminology and notation, the current position corresponds to the following stack of items:

S ← · Expr
Expr ← · Add / Mult / Num
Add ← (( · Mult / Num) '+')+ (Mult / Num)
Mult ← ( · Num '*')+ Num
Num ← · 0-9+

Now we can construct, from this position, a <dfn>retry set</dfn> which includes the non-terminals that may be retried at this same position by any alternative to the right of the current position in each item.
We want to determine which of the current rules' results should be cached, so it is only the current rules which we are interested in.
The current rules are Expr, Add, Mult, Num, i.e. all those which appear immediately following a dot in the above stack.
If a rule succeeds, we will use it to reconstruct the parse tree from the state table, so the question only applies to rules which fail.

Expr appears only once, as the start token, so it will not be tried again and the result need not be cached.
Add does not appear outside of the first alternative in Expr, so it need not be cached.
Mult appears again as the second alternative in Expr, so if it fails we must cache the result.
Num appears again as the third alternative in Expr, so it likewise must be cached.

So the retry set for this position is [Mult, Num].
Num succeeds so we cache the result.
Mult then consumes '*', and tries the +-expr again.

Now we are testing Num again at pos 2.
Although the +-expr is on a different cycle, the retry set happens to be the same, but it could be different since we have matched the +-expr at least once and now the rest of the sequence in which it appears becomes part of the retry set.

Num succeeds at pos 2 and the result is cached.
The second iteration of the +-expr fails at pos 3 since there is no '*'.
The pos is rolled back to 2.
The result table now contains:

0 Expr
0 Add
0 Mult
0 Num    ("2", 1)
2 Num    ("3", 3)

[
An alternative to caching the failure of Mult is to put the parser into a different state.
From this state, by some appropriate state transition table, we could ensure that if the Add alternative of the Expr 0 rule attempt fails, the Mult alternative will not be considered; rather the state transition would be to Expr ← Add / Mult / · Num in that case.
The way to construct such a table would be to consider the retry set of the current state in which the Mult fails, and, if Mult is in this retry set then there must be at least one rule for which Mult appears at a branch to the right of the current position; for each such rule, remove that branch from the rule; create a new state representing this new ruleset, and arrange for the failure of Mult to trigger a transition to this new state.
]

We are now at Mult ← (Num '*')+ · Num and pos = 2.
We look up Num 2 in the result table.

Now we must deal with handling cached positive results.
Conceptually, we have a memoizing recursive descent parser, which returns a parse tree, and so when we look up a result in the table we would return this cached parse tree and insert it into the parse tree the current function is building.
In practice, however, currently what we actually do is notate the parenting information in the result table, such that every positive result contains a pointer to the rule name and position of its parent in the parse tree (at the last time it succeeded).
Thus when a cached positive result is found we simply reparent it in the result table.

The Num 2 having succeeded (by a cache hit), the Mult has also succeeded.
In fact, whenever we are in the final position in two (or more) rules, e.g. Mult ← ... · Num and Num ← · 0-9+, and that final position succeeds, we could think of this as a single state transition which takes several rules off the stack, rather than as a series of individual function calls returning true.

With parenting information, the result table is now:

pos,rule  result     parent
0 Expr
0 Add
0 Mult   ("2*3",3)   O Add
0 Num    ("2", 1)    0 Mult
2 Num    ("3", 1)    0 Mult

We are now at Add ← ((Mult / Num) "+")+ ( · Mult / Num ) and at position 3.
There is no Mult 3 result in the result table, so we create it, and we are now at Mult ← ( · Num "*")+ Num and still at position 2.
As before, it is possible to consider adding both the Mult and Num rules (at these positions) to the stack to be part of a single state transition between the Add ← (( · Mult / Num) '+')+ (Mult / Num) position, and the Add ← ((Mult / Num) '+')+ ( · Mult / Num) position.
We are collapsing everything between one position in a recursive descent parser and the next position at which a terminal is consulted.

The Num rule consumes "4" and succeeds.
We can avoid adding parenting information until the entire parent rule succeeds, saving some needless writes.

pos,rule  result     parent
0 Expr
0 Add
0 Mult   ("2*3",3)   O Add
0 Num    ("2", 1)    0 Mult
2 Num    ("3", 1)    0 Mult
3 Mult
3 Num    ("4", 1)

The Mult 3 now attempts to match a "*" which fails.
The Add 0 then tries the second alternative of the final group, which is a Num.
This is a cached success, so Add 0 succeeds and Num 3 is reparented.
Add 0 was the end of (the first branch of) the Expr rule, so Expr 0 has succeeded and we are done.

The final result table is:

pos,rule  result     parent
0 Expr    2*3+4
0 Add     2*3+4      0 Expr
0 Mult    2*3"       O Add
0 Num     2          0 Mult
2 Num     3          0 Mult
3 Mult    fail
3 Num     4          0 Add

From this we can construct a parse tree in a separate pass, starting with the only parentless success at pos 0, which is Expr.
For each row in the result table we then find the parent reference and attach it to that parent, giving:

0 Expr
 0 Add
  0 Mult
   0 Num
   2 Num
  3 Num

Which is the desired parse tree.  However, this seems a bit inefficient as it requires a separate pass, and we could simply store the child node information as we go, rather than storing parent pointers.

If this is done, the final result table (not showing failed results or matched strings) would be:

pos,rule      children
0 Expr        [0 Add]
0 Add         [0 Mult, 3 Num]
0 Mult        [0 Num, 2 Num]
0 Num         []
2 Num         []
3 Num         []

We can do this by writing child information into the match results as we proceed through a rule, deleting the child list from the result table if the rule eventually fails.
Storing child references has the benefit that, unlike parent information, the child nodes of a successful match at a given position will never change.

Rather than storing the start and end position into the input string of a successful match, for the purposes of incremental parsing we prefer to store the length of the matched string only.
The start position, while shown explicitly in the above result tables, is actually implicitly represented by the row's index within the result table (result rule names are stored in a separate array for each of these position-indexed rows).

In recursive descent or in a packrat parser, the call stack stores both the rules on the stack going back to the start token (which is the call stack itself), and each rule's position within the sequences, ordered choices, repetition operators, etc that make up that rule (which is the code pointer within the execution of that function call on the stack).
If we wish to replace these function calls with an explicit stack we must store both the rule names and the position within each rule.
The code that handles transitions between these states will be ejected from the function bodies into some sort of switch statement.

To represent this stack we can store a rule name and an integer representing a position within that rule.
Since there are a finite number of these states for each rule we could simply use a single integer to represent both the rule and the position within it at the cost of human readability of the generated code.
The child information on what exactly matched could also be kept on the stack but instead we keep this in the result table.

The stack does however also need to store the start position of a rule, before that rule has succeeded and been pushed into the result table.
Another thing we could do is push the rule into the result table when we enter it, not when we know whether it succeeds (this is as shown above).
The stack would then contain a set of pointers into the result table... no, we will use indices.

We actually do keep the child nodes of the in-progress match in the stack as well.
When a rule is added to the stack, if it can have children at all then an array is created to hold them.

If any branch fails, we must remove any children which that branch contributed to the rule's child array.
I think there may even be a bug here in the current code, I don't think we handle this at all, e.g. a rule A ← B C / D, when B succeeds, C fails, and D succeeds, I think a B child of A will appear in the generated parse tree.
we have a checkpoint() and restore() functions on the state object, which are used in sequences, but they restore only the position, not any parenting information.
In fact, testing now reveals this bug does in fact exist.
For whatever reason it had not previously been discovered.

For our purposes here a revocable branch is any part of a rule that can be backtracked over and that contributes child nodes to the rule before possibly failing.

S ← A B / C ; here A B is a revocable branch
S ← (A B)+ ; for any of *, ?, or + operators, such a rule can produce an A child that needs to be revoked if B fails.

For a given rule we wish to identify a set of sequence or branch points at which we will store the current number of children of this rule.
If the sequence fails we will then truncate the child node sequence back to this number.
At other points, e.g. in the rule S ← A B / C, we do not need to store any number, as the number of children to roll back to will always be (in this case) zero.
(In a case like S ← A (B C / D), the number would be one, but still statically known.)
If there is a sequence, we may need to roll back to a number of children that is not statically known.
Since we are not relying on recursive descent, we do not want to make the rollback dependent on the code which did the failed matches.
Rather we must save as we go all data necessary to do any rollback that might become necessary.

We can use the existing stack machinery to do this, and simplify the design, by adding additional states for each branch point in each rule and pushing these onto the stack.
This also allows us to easily reset the current position in any part of a rule which may be rolled back.

A <dfn>branch</dfn> of a rule is defined as follows:
For any rule having an OrdChoice with more than one Sequence child, each child is a branch.
For any sequence which contains a repetition expression, the repetition expression, and the rest of the sequence, are branches.
Any positive or negative lookahead is a branch.

Ordered choice, repetition, and lookahead operators are the three ways that backtracking is introduced in a PEG.
We can deal with each of these in the same way and keep the backtracking machinery simple.

We distinguish branches which can fail without the entire rule failing, from those which cannot.
The last branch in a rule cannot fail without the rule failing, so there is no need to store rollback information.
(Put differently, the retry set that we can construct while staying within that rule is empty.)
We call such branches soft-failing.
A hard-failing branch, in contrast, is one which, if it fails, implies the failure of the entire rule.

Of those branches that remain, we distinguish those which can add children to the rule but then still fail from those which cannot.
An ordered choice is child-polluting if its last alternative is.
A sequence which contains at least one non-terminal followed by any expression which may fail is child-polluting.
A repetition is child-polluting if it contains any non-terminal.
Lookaheads are not child-polluting, since they cannot add children regardless of whether or not they succeed.

We can also distinguish branches which advance the potentially advance the position from those which do not.
Most branches in any practical grammar will potentially consume some input before failing, so we simply regard storing the current input position as necessary in all soft-failing branches (and indeed we store this anyway, as a key into the result table and a way to distinguish different applications of the same rule).

The context in which a branch appears determines whether we need to add an additional stack frame to support the rollback of that branch.
A top-level branch is one which, when rolled back, leaves the rule in a position of having matched nothing.
A top-level branch can be rolled back by simply resetting pos to the position of the rule application, and resetting the rule child sequence to the initial empty state.
In other words we replace the frame on the top of the stack with a new frame corresponding to the remainder of the rule as if it were being matched afresh.
For any other branch, there is a branch return point, at which we conceptually take a checkpoint of the state of the rule application to which we will return if the branch fails.
This checkpoint includes the position and the child node sequence (which we can simply store as a length).

For any A, we can treat A+ as A A*, for the purposes of branch analysis.
More generally, any repetition from m to n times may be similarly reconstructed as a mandatory part followed by a part which may succeed zero or more (possibly bounded) times.
We regard the Add rule, then, as:
Add ← ((Mult / Num) '+') ((Mult / Num) '+')* (Mult / Num)

This makes clear that the first child expression of the rule, "((Mult / Num) '+')", is not a branch, as it is mandatory for the rule to succeed.
The next expression does introduce a branch as it may succeed zero or more times.
Any given test of this expression is soft-failing, and the expression is child-polluting since it contains "(Mult / Num)" followed by "'+'", which may fail.
It is not a top-level branch, because rolling back to the beginning of the rule if this branch fails would be incorrect.
Therefore on the basis of all these criteria this is a branch which must result in a checkpoint of both the child sequence and the current position.
Like all repetitions, which are always greedy in a PEG, at most one failure of the sub-expression will need to be rolled back, thus every time we succeed in matching a sub-expression it is safe to overwrite the previous checkpoint information with the current state.

Add contains a branch-checkpoint, as does Mult, but Num does not (even though it contains a +-expr) since "0-9" is not child-polluting, and does not advance the position if it fails.

Each soft-failing expression in a rule needs a way to propagate success or failure back up the stack.
We could do this by having a return value, which the next state in the parent rule would then consult, but instead we will use two states in the parent.
In other words, for a position A ← α · β, where β is an expression which may fail, we create two new states.
The first is the state β · in which β has succeeded, while the second new state represents the failure of β.
Actually it may be more convenient to number these in reverse.

Numbering the positions within a rule:
Add ← ((Mult / Num) '+')+ (Mult / Num)
First we replace any {m,n} repetition with m repetitions followed by a {0,n-m} repetition.
Add ← ((Mult / Num) '+') ((Mult / Num) '+'){0,0} (Mult / Num)
Next we find the leftmost atomic expression (either a non-terminal or a terminal expression).
This is position 0.
Using a dot to mark this position:
Add ← (( · Mult / Num) '+') (Mult / Num) '+'){0,0} (Mult / Num)
Since this is followed a non-terminal, we add two further states, the first is one in which it has succeeded, in the second it has failed.
We give the failure state position n+1 and the success state n+2.

Expr ← Add / Mult / Num
Add ← ((Mult / Num) '+')+ (Mult / Num)
Mult ← (Num '*')+ Num
Num ← 0-9+

In code, the above grammar:

function parseExpr(s){var tbl,l,pos,c,stack,x,match_name,match_pos,cached
 l=s.length
 tbl=Table(l)
 stack=[[0,'Expr',0,[]],[0,'Add',0,[]],[0,'Mult',0,[]],[0,'Num',0]] // pre-fill the stack up to the first input test
 state=['Num',0] // this is basically the top of the stack (always the same rule, not always the same position)
 pos=0
 for(;;){
  c=s.charCodeAt(pos) // here (not shown) we can deal with UTF-16 in just one place (and with just one < comparison in the BMP case)
  switch(state){
  case ['Num',0]: // obviously this is not JavaScript, assume these pairs are replaced by a single state integer
   if(c<48?false:c<58?true:false) // if c in 0-9
    return [false,tbl] // we can prove that not matching Num here is fatal, see explanation below
   pos++
   state=['Num',1] // Num,1 is the state after the +-expr has matched once, so success is guaranteed but we have to keep trying the +-expr.
   break
  case ['Num',1]:
   if(/* c in 0-9 as above */)
    pos++ // we want to stay in state ['Num',1]
   else
    // the +-expr has succeeded at least once and has also failed once.
    // that means we're done with the Num rule, so we need to pop the stack and cache the result
    x=stack.pop()
    [match_pos,match_name]=x
    tbl[match_pos][match_name]=[true,pos-x[0],[]] // cache a positive result
    stack[stack.length-1][3].push(x) // we push this result onto the child sequence of the previous stack object
    // now we must transition to a new state.
    // however this is the end of the Num rule so the state depends on what is in the stack.
    // we simply increment the state of the parent stack object, and rely on this new state to handle whatever happens next.
    // this avoids a combinatorial explosion in the number of distinct states, and is similar to how LR state transitions are handled.
    state=stack[stack.length-1]
    state[2]++ // increment the position within the parent rule.
    // this is equivalent to 'return true' causing control to pass to the next statement in the caller in a recursive descent parser.
    // rules like Num which always end by consuming a terminal do not need a special 'ending' state, but rules like Mult do.
    break
  case ['Mult',1]: // (Num · '*') (Num '*')* Num
   if(c==42){ // c is '*'
    // we are moving into a branch-checkpoint, we will return here if '*' in (Num '*')* fails to match on the next iteration.
    // each time it succeeds we will replace this with a new checkpoint.
    stack.push([pos,'Mult',2,[]])
    state=['Mult',2]}
   else{ // the entire Mult rule has failed
    stack.pop() // throw away the Mult stack object
    // now we need some way to signal failure to the parent object in the stack
    state=stack[stack.length-1]
    state[2]+=2 // increasing the position within the parent rule by two indicates failure
   }}}

function Table(l){var a=[]
 while(l--)a.push([])
 return a}


-----------------
Simple v5 codegen
-----------------

Simplifying the above plans in the interests of expediency, we construct a set of rules to generate code from any rule.

We explicitly manipulate a stack, avoiding function call overhead.

We construct from a ruleset a set of states, and a switch statement loops, executing the code for the current state, until the input is either parsed or an error is generated.

We first aim to complete the simplest correct solution within these constraints, and can optimize for performance later.

Each parser rule creates a stack object, and further stack objects are created by some expressions within rules.

When a stack object is pushed onto the stack it becomes the current state.
When an expression fails, it increments the previous state, if it succeeds it increases the previous state by two.

Each rule that contains a subexpression must then handle both the states which may result.
Introducing a subexpression, then, creates three new states.

A sub-expression is "called" by pushing that state on the stack.
If the state is N when the sub-expression state is pushed, the next state after the subexpression will be either N+1 or N+2.

The overhead at runtime of introducing a new state object is manipulation of the stack, and setting and comparing the state value during the extra iteration through the loop and switch statement.
At this point then we can introduce these states freely and only later streamline the unnecessary states.

A parser expression is either a string literal, a character set, a sequence, an ordered choice, a repetition (characterized by two integer limits), or a negative or positive lookahead.
To simplify string literal and character set handling, we convert all string literals to a sequence of singleton character sets.
Later we can add optimizations which will then benefit such sequences regardless of how they were specified in the grammar.

For a character set we generate a state which determines whether the character at the current position matches that set.

For a sequence, we generate a state S for each expression in the sequence.
We then generate a sequence state A, which is the state which begins a match against this sequence.
State A simply sets the state to the first expression state S.
It also pushes the current position onto the stack (or possibly, every stack object has this).
A+1, the failure state for this expression, pops the stack until A is at the top of the stack, discarding everything it pops.
It then fails.
A+2, the success state for the first sub-expression, sets the state to the next expression state.
Each sub-expression failure state does the same thing described above.
The final sub-expression success state pops the stack, collecting all child nodes, until A is at the top of the stack.
It then removes A from the stack, adds the child nodes back onto the stack, and succeeds.

For an ordered choice, we generate a state for each sub-expression.
The initial state of the ordered choice sets the state to the first sub-expression.
The failure state sets the state to the next sub-expression.
The success state succeeds, leaving any children on the stack.

For a repetition characterized by m minimum and n maximum (or unlimited if n=0) repetitions, we generate max(m,n) individual states as follows:
The first m states each set the state to the subexpression.
The failure state fails, the success state continues to the next state within the repetition.
After the m required states we have n-m optional states if n is non-zero.
In each of these, the failure state succeeds.
In each but the last, the success state proceeds to the next of the repetition states.
In the last, the success state succeeds.
In case n = 0, the required states (if any) are followed by an unlimited repetition state.
The failure state succeeds.
The success state sets the state to the unlimited repetition state again.

A positive lookahead generates a state which sets the state to the subexpression.
When the subexpression succeeds, the lookahead pops all children from the stack and then succeeds.
When the subexpression fails the lookahead fails.

A negative lookahead is similar but with the success condition reversed.

For each rule we generate a begin state which tests the result table for this rule name at the current position.
If found, the rule succeeds or fails as appropriate.
The child nodes, if any, of a successful match will already exist in the result table, so no copying is necessary.
If a cached result is not found, we transition to the state of the expression on the RHS of this rule.
In the failure state


-----------------
Actual v5 codegen
-----------------

Some cleanups, function inlining, and reworking the state object and the way it is updated gave remarkable speed improvements.
Very basic benchmarking results showed that the function call overhead of JavaScript is entirely a myth at this point.
In Firefox 3.5, at least some function calls are actually much faster than an equivalent loop.

Benchmarking the various codegen iterations on the same input:

p_PEG    true 15820
p_PEG_v1 true 13239
p_PEG_v3 true 4981
p_PEG_v4 true 5088
p_PEG_v5 true 1341

So we have about a 10x speedup over the original code.
None of the above was actually done, but some of it would still be useful.
In particular, analyzing rules which do not need to be cached in the result table would speed things up further.
Rather like ES5, this v5 turned out quite differently than expected... I think I narrowly dodged a bullet... a near-complete rewrite is wholly unnecessary at this time.

20090901

Currently, parsing peg.js with the ES5 grammar and the v5 codegen (without building a tree) takes about 12 seconds.
Removing all the result table lookups and insertions changes the runtime to ... about 12 seconds.
(This has the side effect of making it impossible to build the parse tree since we generate it from the table.)
This suggests that most of the time is spent in actual parsing mechanics, not in result table management overhead.
Profiling the non-tabulated code shows 7% of the time in a single parse rule (LineTerminatorSequence) which is called 79000 times on input of < 9000 characters.
g(), the function which advances to the next character in the input, was called 620727 times, indicating that a lot of backtracking is happening (i.e. the entire input could have been covered about 70 times).
S, SingleLineComment, MultiLineComment are also called > 70000 times and responsible for a large portion of parse time.
t, the truncate function, is called 537167 times, another indication of lots of backtracking.
The profiled version took 42s to run, considerably slower than the raw 12s seen without profiling.
...oddly... the version with tabulation runs faster with the profiler running than in does in 3box without the profiler, by a factor of > 3.
actually holy shit it's more than a factor of 5, 3 seconds vs 17.
It's because of the Worker and the way we pass the result data back.
To save time (haha) I had replaced the call to b(), the tree builder, and just returned the result table, but the profiler reveals, *more than half the time* is in util.js, serializing and deserializing the result.
it now runs in 5 seconds, not 17, and that is with the profiler running.
I was misinterpreting the profiler results to some extent, as the profiler apparently does not even see worker code at all.
Now, with the tabulating code, 16% of the runtime is in fin() which is recording results in the table many of which we can prove will never be looked up.
It is called 84000 times, which suggests about 10 rules are being tried per character, which isn't that terribly inefficient but could be improved.
3% is in t, called 44000 times.
Switching to a more efficient array method (.splice() instead of repeated .pop()) made this much worse, now 7% of runtime.
Using both .splice() and a test (i.e. if(a.length>n)a.splice(n)) made it marginally better.
LineTerminatorSequence is now called 2617 times, an improvement from 79000.
g() is now called 26797 times, down from 620727, indicating that backtracking has been greatly reduced.
Changing g() to take pos as an argument rather than access the pos in the outer scope, and to not declare a useless internal variable, reduced its contribution by about half.

20090906

Generating a profiling version of the parser which logs all puts to the result table and all hits (lookup misses are equal to puts), and running that version over the jQuery library and compiling the results, hiding all rules with 0 hits, gives the following:

[{name:"ElseTok"
 ,puts:427
 ,hits:1
 ,pos:0
 ,neg:1
 ,ratio:0.00234192037470726}
,{name:"IdentifierStart"
 ,puts:52587
 ,hits:341
 ,pos:0
 ,neg:341
 ,ratio:0.006484492365033183}
,{name:"SQ"
 ,puts:3833
 ,hits:43
 ,pos:43
 ,neg:0
 ,ratio:0.01121836681450561}
,{name:"VarTok"
 ,puts:2535
 ,hits:35
 ,pos:35
 ,neg:0
 ,ratio:0.013806706114398421}
,{name:"LineTerminatorSequence"
 ,puts:29636
 ,hits:778
 ,pos:0
 ,neg:778
 ,ratio:0.02625185585099204}
,{name:"Comment"
 ,puts:24846
 ,hits:778
 ,pos:0
 ,neg:778
 ,ratio:0.03131288738629961}
,{name:"RS"
 ,puts:18363
 ,hits:647
 ,pos:11
 ,neg:636
 ,ratio:0.035233894243859934}
,{name:"IdentifierPart"
 ,puts:42050
 ,hits:1705
 ,pos:206
 ,neg:1499
 ,ratio:0.04054696789536266}
,{name:"MultiLineCommentNoLB"
 ,puts:5945
 ,hits:294
 ,pos:0
 ,neg:294
 ,ratio:0.04945332211942809}
,{name:"DQ"
 ,puts:8193
 ,hits:593
 ,pos:593
 ,neg:0
 ,ratio:0.0723788600024411}
,{name:"Identifier"
 ,puts:9415
 ,hits:688
 ,pos:0
 ,neg:688
 ,ratio:0.07307488050982475}
,{name:"SnoLB"
 ,puts:6239
 ,hits:918
 ,pos:0
 ,neg:918
 ,ratio:0.14713896457765668}
,{name:"WhiteSpace"
 ,puts:48904
 ,hits:8929
 ,pos:2242
 ,neg:6687
 ,ratio:0.18258220186487814}
,{name:"SingleLineComment"
 ,puts:25239
 ,hits:5842
 ,pos:0
 ,neg:5842
 ,ratio:0.23146717381829707}
,{name:"Elision"
 ,puts:121
 ,hits:38
 ,pos:0
 ,neg:38
 ,ratio:0.3140495867768595}
,{name:"VariableDeclarationNoIn"
 ,puts:73
 ,hits:35
 ,pos:35
 ,neg:0
 ,ratio:0.4794520547945205}
,{name:"FunctionTok"
 ,puts:3412
 ,hits:1892
 ,pos:17
 ,neg:1875
 ,ratio:0.5545134818288394}
,{name:"LeftHandSideExpr"
 ,puts:8458
 ,hits:6313
 ,pos:4429
 ,neg:1884
 ,ratio:0.7463939465594703}
,{name:"CR"
 ,puts:31122
 ,hits:25140
 ,pos:0
 ,neg:25140
 ,ratio:0.8077887025255446}
,{name:"MemberExpr"
 ,puts:8466
 ,hits:7572
 ,pos:5218
 ,neg:2354
 ,ratio:0.8944011339475549}
,{name:"NewTok"
 ,puts:2359
 ,hits:2354
 ,pos:3
 ,neg:2351
 ,ratio:0.9978804578211107}
,{name:"DecimalIntegerLiteral"
 ,puts:4173
 ,hits:4172
 ,pos:464
 ,neg:3708
 ,ratio:0.9997603642463455}
,{name:"ForTok"
 ,puts:1093
 ,hits:3222
 ,pos:129
 ,neg:3093
 ,ratio:2.947849954254346}
,{name:"S"
 ,puts:25092
 ,hits:87838
 ,pos:32050
 ,neg:55788
 ,ratio:3.500637653435358}]

Totals:
{puts:885152
,hits:160168
,pos:45475
,neg:114693}

This shows that most of the tokens are getting zero hits in the result table.

Running the same test on a small file of mine (API.js) gives:

[ ... ]
,{name:"IdentifierStart"
 ,puts:1180
 ,hits:3
 ,pos:0
 ,neg:3
 ,ratio:0.002542372881355932}
,{name:"IdentifierPart"
 ,puts:938
 ,hits:39
 ,pos:0
 ,neg:39
 ,ratio:0.04157782515991471}
,{name:"DQ"
 ,puts:172
 ,hits:8
 ,pos:8
 ,neg:0
 ,ratio:0.046511627906976744}
,{name:"Identifier"
 ,puts:268
 ,hits:16
 ,pos:0
 ,neg:16
 ,ratio:0.05970149253731343}
,{name:"Comment"
 ,puts:509
 ,hits:32
 ,pos:0
 ,neg:32
 ,ratio:0.06286836935166994}
,{name:"RS"
 ,puts:586
 ,hits:41
 ,pos:10
 ,neg:31
 ,ratio:0.06996587030716724}
,{name:"SQ"
 ,puts:271
 ,hits:23
 ,pos:23
 ,neg:0
 ,ratio:0.08487084870848709}
,{name:"MultiLineCommentNoLB"
 ,puts:169
 ,hits:16
 ,pos:0
 ,neg:16
 ,ratio:0.09467455621301775}
,{name:"LineTerminatorSequence"
 ,puts:604
 ,hits:70
 ,pos:25
 ,neg:45
 ,ratio:0.11589403973509933}
,{name:"SingleLineComment"
 ,puts:555
 ,hits:134
 ,pos:0
 ,neg:134
 ,ratio:0.24144144144144145}
,{name:"WhiteSpace"
 ,puts:765
 ,hits:230
 ,pos:29
 ,neg:201
 ,ratio:0.3006535947712418}
,{name:"Elision"
 ,puts:12
 ,hits:4
 ,pos:0
 ,neg:4
 ,ratio:0.3333333333333333}
,{name:"SnoLB"
 ,puts:185
 ,hits:96
 ,pos:0
 ,neg:96
 ,ratio:0.518918918918919}
,{name:"CR"
 ,puts:771
 ,hits:525
 ,pos:0
 ,neg:525
 ,ratio:0.6809338521400778}
,{name:"MemberExpr"
 ,puts:206
 ,hits:168
 ,pos:124
 ,neg:44
 ,ratio:0.8155339805825242}
,{name:"LeftHandSideExpr"
 ,puts:203
 ,hits:168
 ,pos:127
 ,neg:41
 ,ratio:0.8275862068965517}
,{name:"FunctionTok"
 ,puts:69
 ,hits:60
 ,pos:11
 ,neg:49
 ,ratio:0.8695652173913043}
,{name:"NewTok"
 ,puts:46
 ,hits:44
 ,pos:1
 ,neg:43
 ,ratio:0.9565217391304348}
,{name:"DecimalIntegerLiteral"
 ,puts:95
 ,hits:95
 ,pos:13
 ,neg:82
 ,ratio:1}
,{name:"ForTok"
 ,puts:43
 ,hits:129
 ,pos:0
 ,neg:129
 ,ratio:3}
,{name:"S"
 ,puts:536
 ,hits:2310
 ,pos:473
 ,neg:1837
 ,ratio:4.309701492537314}]

Totals:
{puts:22473
,hits:4211
,pos:844
,neg:3367}

This looks fairly consistent with the jQuery results at least to a quick look.

So after running these profiles we can run the codegen again passing the list of rules that were never hit once in our tests, and benchmark these results against the un-profiled codegen.

Letting the profiler output the zero-hit rules gives, for the API.js case (with number of puts in parentheses):

UnicodeLetter (1180)
SourceCharacter (1147)
LF (850)
LS (771)
PS (771)
MultiLineComment (509)
Keyword (268)
ReservedWord (268)
LineTerminator (246)
FutureReservedWord (233)
IdentifierName (232)
ThisTok (206)
PrimaryExpr (206)
CallExpr (203)
Arguments (199)
SingleStringCharacter (197)
PostfixExpr (193)
UnaryExpr (193)
MultiplicativeExpr (192)
AssignmentExpr (178)
AdditiveExpr (175)
ShiftExpr (175)
RelationalExpr (175)
EqualityExpr (172)
BitwiseAndExpr (170)
BitwiseXOrExpr (170)
BitwiseOrExpr (170)
NewExpr (168)
LogicalAndExpr (168)
LogicalOrExpr (168)
ConditionalExpr (168)
MultiplicativeOp (150)
AdditiveOp (150)
AssignmentOperator (137)
ShiftOp (133)
InstanceOfTok (133)
InTok (133)
RelationalOp (133)
EqualityOp (133)
NullTok (99)
NullLiteral (99)
TrueTok (99)
BooleanLiteral (99)
Literal (99)
FalseTok (97)
DecimalLiteral (95)
NumericLiteral (95)
DoubleStringCharacter (90)
Expr (84)
HexIntegerLiteral (82)
StringLiteral (82)
Block (74)
Statement (74)
VarTok (70)
VariableStatement (70)
EmptyStatement (60)
ExprStatement (60)
RegularExpressionLiteral (51)
ArrayLiteral (51)
IfTok (48)
IfStatement (48)
ObjectLiteral (47)
FunctionExpr (46)
DoTok (43)
DoWhileStatement (43)
WhileTok (43)
WhileStatement (43)
ForInStatement (43)
ForStatement (43)
IterationStatement (43)
ContinueTok (43)
ContinueStatement (43)
BreakTok (43)
BreakStatement (43)
ReturnTok (43)
ReturnStatement (43)
DeleteTok (43)
VoidTok (43)
TypeofTok (43)
ArgumentList (38)
EOS (38)
LineContinuation (31)
WithTok (27)
WithStatement (27)
LabelledStatement (27)
SwitchTok (27)
SwitchStatement (27)
ThrowTok (27)
ThrowStatement (27)
TryTok (27)
TryStatement (27)
DebuggerTok (27)
DebuggerStatement (27)
FunctionDeclaration (23)
VariableDeclaration (19)
EOF (16)
EOSnoLB (16)
ExponentPart (13)
FormalParameterList (11)
FunctionBody (11)
SingleEscapeCharacter (10)
CharacterEscapeSequence (10)
EscapeSequence (10)
DecimalDigit (7)
ElseTok (5)
ElementList (4)
Program (1)

And for jQuery:

UnicodeLetter (52587)
LF (35618)
LS (31122)
PS (31122)
MultiLineComment (24846)
IdentifierName (10413)
Keyword (9415)
ReservedWord (9415)
ThisTok (8466)
PrimaryExpr (8466)
CallExpr (8458)
PostfixExpr (7952)
UnaryExpr (7952)
FutureReservedWord (7928)
MultiplicativeExpr (7694)
NewExpr (7572)
AdditiveExpr (7540)
ShiftExpr (7540)
RelationalExpr (7371)
Arguments (7153)
EqualityExpr (6995)
BitwiseAndExpr (6773)
BitwiseXOrExpr (6770)
AssignmentExpr (6656)
SourceCharacter (6631)
BitwiseOrExpr (6580)
LogicalAndExpr (6363)
LogicalOrExpr (6173)
ConditionalExpr (6173)
LineTerminator (5982)
MultiplicativeOp (5615)
AdditiveOp (5599)
ShiftOp (5445)
RelationalOp (5384)
InstanceOfTok (5378)
EqualityOp (5376)
InTok (5375)
AssignmentOperator (4912)
DoubleStringCharacter (4476)
NullTok (4339)
NullLiteral (4339)
Literal (4339)
TrueTok (4277)
BooleanLiteral (4277)
FalseTok (4206)
DecimalLiteral (4173)
NumericLiteral (4173)
StringLiteral (3717)
HexIntegerLiteral (3708)
Expr (3670)
RegularExpressionLiteral (3055)
ArrayLiteral (2972)
ObjectLiteral (2895)
Block (2805)
Statement (2775)
FunctionExpr (2681)
VariableStatement (2481)
DeleteTok (2337)
VoidTok (2329)
TypeofTok (2329)
EmptyStatement (2267)
ExprStatement (2266)
IfTok (1543)
IfStatement (1543)
EOS (1231)
DoTok (1115)
DoWhileStatement (1115)
WhileTok (1115)
WhileStatement (1115)
IterationStatement (1115)
ForInStatement (1093)
ForStatement (1071)
ContinueTok (1031)
ContinueStatement (1031)
BreakTok (1029)
BreakStatement (1029)
ReturnTok (1012)
ReturnStatement (1012)
RegularExpressionChar (1010)
ArgumentList (956)
SingleStringCharacter (709)
WithTok (705)
WithStatement (705)
LabelledStatement (705)
SwitchTok (705)
SwitchStatement (705)
ThrowTok (704)
ThrowStatement (704)
TryTok (700)
TryStatement (700)
DebuggerTok (685)
DebuggerStatement (685)
LineContinuation (636)
ExponentPart (465)
PropertyName (379)
PropertyAssignment (379)
FunctionDeclaration (357)
VariableDeclaration (352)
FormalParameterList (339)
FunctionBody (339)
EOSnoLB (330)
EOF (294)
RegularExpressionBackslashSequence (292)
DecimalDigit (269)
RegularExpressionClassChar (220)
RegularExpressionClass (122)
AssignmentExprNoIn (101)
RelationalExprNoIn (100)
EqualityExprNoIn (100)
BitwiseAndExprNoIn (100)
BitwiseXOrExprNoIn (100)
BitwiseOrExprNoIn (100)
LogicalAndExprNoIn (100)
LogicalOrExprNoIn (100)
ConditionalExprNoIn (100)
PropertyNameAndValueList (94)
RegularExpressionFirstChar (83)
RegularExpressionBody (83)
RegularExpressionFlags (83)
RelationalOpNoIn (61)
ExprNoIn (40)
ElementList (38)
VariableDeclarationListNoIn (35)
CatchTok (15)
Catch (15)
FinallyTok (15)
Finally (15)
SingleEscapeCharacter (11)
CharacterEscapeSequence (11)
EscapeSequence (11)
CaseTok (5)
CaseClause (5)
DefaultTok (1)
DefaultClause (1)
Program (1)

Removing all of these from the result table (entirely) gives a speedup of somewhere around 40%.

--------
20100410
--------

----------
v6 codegen
----------

The v6 codegen is now finished and the default codegen.

It uses a table-based parsing approach rather than recursive descent, and does not use native regexes.
It is currently unoptimized, and slower than the v5 parsers, which also did not use regexes, but did use recursive descent.
One of the first optimizations I plan to port is collapsing regular subparts of the grammar into single expressions.
Rather than use native regexes however, I will be compiling these to DFAs.
Later, sets of regular parts can be combined into an NFA, allowing tokenization to proceed in a single pass over the input in many cases.

To make this optimization in many cases we will want to eliminate rules from the parse tree that would prevent these simplifications.
For example many rules in the ES5 grammar are effectively simply names for character classes (e.g. IdentifierStart, IdentifierPart), and do not belong in parse trees (except perhaps when debugging the grammar).

We add options to elide a rule and to drop a rule.
Eliding a rule simply removes the rule itself from the parse tree, but any child nodes will still appear.
Dropping a rule drops that match and any child matches from the tree, somewhat like a forward lookahead if it also consumed the input.
Dropping a rule is like eliding it and all its children.
Eliding a rule is like inlining the rule definition at the point of use in the grammar.

If a rule is elided or dropped, we only care about the parse result, not any actions or child nodes that might otherwise be produced.
This means we can compile a DFA that does none of these things, and use that to test that parser state instead of using the normal parsing strategy.
An expression can be compiled to a DFA if it is regular.
There can be expressions that match regular languages, but cannot be identified as regular by the heuristics we use.
For example, a PEG could be implemented using recursion where recursion is not necessary, and our heuristics would give up even if the matched language is trivially regular.
So our heuristics can tell us in some cases that an expression is regular, but cannot tell us that an expression is not.

Our heuristic is:
- csets are regular
- sequences are regular if all elements are regular
- ordC is regular if all elements are regular
- repetitions and lookaheads are regular if the sub-expression is
- named references are regular if the rule is regular and is not part of a dependency cycle